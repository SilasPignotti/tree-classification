{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RQ4.1:** Wie hoch ist die maximale erreichbare Performance auf Berlin?\n",
        "\n",
        "**RQ4.2:** Welche Hyperparameter-Konfiguration optimiert XGBoost f√ºr Single-City?\n",
        "\n",
        "**RQ4.3:** Wie viele Trainingssamples werden f√ºr optimale Performance ben√∂tigt?\n",
        "\n",
        "**RQ4.4:** Welche Features/Monate sind kritisch f√ºr die Klassifikation?\n",
        "\n",
        "**RQ4.5:** Welche Genera sind schwer separierbar (Confusion Patterns)?\n",
        "\n",
        "**RQ4.6:** Gibt es systematische Performance-Unterschiede zwischen Deciduous/Coniferous?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard Libraries\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ML Libraries\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Publication Style\n",
        "PUBLICATION_STYLE = {\n",
        "    'style': 'seaborn-v0_8-whitegrid',\n",
        "    'figsize': (12, 6),\n",
        "    'dpi_export': 300\n",
        "}\n",
        "\n",
        "def setup_publication_style():\n",
        "    plt.rcdefaults()\n",
        "    plt.style.use(PUBLICATION_STYLE['style'])\n",
        "    sns.set_palette('Set2')\n",
        "    plt.rcParams['figure.figsize'] = PUBLICATION_STYLE['figsize']\n",
        "    plt.rcParams['savefig.dpi'] = PUBLICATION_STYLE['dpi_export']\n",
        "    print(\"Publication Style configured\")\n",
        "\n",
        "setup_publication_style()\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"‚úÖ Imports successful\")\n",
        "print(f\"XGBoost Version: {xgb.__version__}\")\n",
        "print(f\"Pandas Version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration & Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_CONFIG = {\n",
        "    \"city\": \"berlin\",\n",
        "    \"buffer\": \"20m_edge\",\n",
        "    \"features\": \"top50\",\n",
        "    \"chm\": \"no_chm\",  # Phase 0 decision\n",
        "    \"target_genera\": 6,\n",
        "    \"train_samples_full\": 242000,\n",
        "    \"val_samples\": 62000,\n",
        "    \"subsample_size\": 50000  # For Experiments 4.1 & 4.2\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD SELECTED FEATURES\n",
        "# =============================================================================\n",
        "\n",
        "with open(SELECTED_FEATURES_PATH, 'r') as f:\n",
        "    selected_features_config = json.load(f)\n",
        "\n",
        "SELECTED_FEATURES = selected_features_config['selected_features']\n",
        "print(f\"‚úÖ Loaded {len(SELECTED_FEATURES)} selected features\")\n",
        "\n",
        "# =============================================================================\n",
        "# LOAD BERLIN DATA\n",
        "# =============================================================================\n",
        "\n",
        "# Train data\n",
        "train_path = INPUT_DATA_DIR / f\"berlin_20m_edge_top50_train.parquet\"\n",
        "df_train = pd.read_parquet(train_path)\n",
        "\n",
        "# Validation data\n",
        "val_path = INPUT_DATA_DIR / f\"berlin_20m_edge_top50_val.parquet\"\n",
        "df_val = pd.read_parquet(val_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded Berlin data\")\n",
        "print(f\"Train: {len(df_train):,} samples\")\n",
        "print(f\"Val: {len(df_val):,} samples\")\n",
        "\n",
        "# =============================================================================\n",
        "# DATA VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n=== Data Validation ===\")\n",
        "\n",
        "# Check for missing values\n",
        "train_missing = df_train[SELECTED_FEATURES].isnull().sum().sum()\n",
        "val_missing = df_val[SELECTED_FEATURES].isnull().sum().sum()\n",
        "print(f\"Missing values - Train: {train_missing}, Val: {val_missing}\")\n",
        "\n",
        "if train_missing > 0 or val_missing > 0:\n",
        "    raise ValueError(\"‚ùå Missing values detected! Run Feature Engineering pipeline first.\")\n",
        "\n",
        "# Check genus distribution\n",
        "print(\"\\n=== Genus Distribution (Train) ===\")\n",
        "genus_counts = df_train['genus_latin'].value_counts()\n",
        "for genus, count in genus_counts.items():\n",
        "    pct = count / len(df_train) * 100\n",
        "    print(f\"{genus:12s}: {count:6,} ({pct:5.2f}%)\")\n",
        "\n",
        "print(\"\\n‚úÖ Data validation complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PREPARE FEATURES & TARGET\n",
        "# =============================================================================\n",
        "\n",
        "# Create subsample for Experiments 4.1 & 4.2\n",
        "df_train_subsample = df_train.sample(\n",
        "    n=DATASET_CONFIG['subsample_size'],\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "print(f\"\\n‚úÖ Created subsample: {len(df_train_subsample):,} samples\")\n",
        "\n",
        "# Extract features (full dataset)\n",
        "X_train_full = df_train[SELECTED_FEATURES].values\n",
        "X_val = df_val[SELECTED_FEATURES].values\n",
        "\n",
        "# Extract features (subsample)\n",
        "X_train_sub = df_train_subsample[SELECTED_FEATURES].values\n",
        "\n",
        "# Extract target (full dataset)\n",
        "y_train_full_raw = df_train['genus_latin'].values\n",
        "y_val_raw = df_val['genus_latin'].values\n",
        "\n",
        "# Extract target (subsample)\n",
        "y_train_sub_raw = df_train_subsample['genus_latin'].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train_full_raw)  # Fit on full to ensure all genera\n",
        "y_train_sub = label_encoder.transform(y_train_sub_raw)\n",
        "y_val = label_encoder.transform(y_val_raw)\n",
        "\n",
        "# Normalize features - SUBSAMPLE (for Experiments 4.1 & 4.2)\n",
        "scaler_sub = StandardScaler()\n",
        "X_train_sub_scaled = scaler_sub.fit_transform(X_train_sub)\n",
        "X_val_scaled = scaler_sub.transform(X_val)\n",
        "\n",
        "print(\"\\n‚úÖ Features prepared\")\n",
        "print(f\"Train (subsample) shape: {X_train_sub_scaled.shape}\")\n",
        "print(f\"Val shape: {X_val_scaled.shape}\")\n",
        "print(f\"Genera: {list(label_encoder.classes_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Experiment 4.1: Baseline Performance (Subsample)\n",
        "\n",
        "**Zweck:** Etablierung der Phase 1 Baseline mit 50k Subsample als Referenz.\n",
        "\n",
        "**Methodik:**\n",
        "- Hyperparameter: Phase 1 Config (max_depth=8, lr=0.1, reg_alpha=0.1, reg_lambda=1)\n",
        "- Data: Berlin train 50k subsample + val (62k full)\n",
        "- Metrics: Train/Val F1, Gap, Per-Genus Performance\n",
        "\n",
        "**Expected:** Val F1 ~0.58 (Phase 1 hatte 0.5805 auf 50k subsample)\n",
        "\n",
        "**Rationale:** Schnelles Baseline-Establishment f√ºr HP-Tuning ohne lange Trainingszeiten.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAIN BASELINE MODEL\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=== Training Baseline Model (Phase 1 HPs, 50k Subsample) ===\")\n",
        "\n",
        "baseline_model = xgb.XGBClassifier(**PHASE1_HP)\n",
        "baseline_model.fit(X_train_sub_scaled, y_train_sub)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = baseline_model.predict(X_train_sub_scaled)\n",
        "y_val_pred = baseline_model.predict(X_val_scaled)\n",
        "\n",
        "# Metrics\n",
        "train_f1 = f1_score(y_train_sub, y_train_pred, average='macro')\n",
        "val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
        "gap = (train_f1 - val_f1) / train_f1 * 100\n",
        "\n",
        "print(f\"\\n‚úÖ Baseline Training Complete\")\n",
        "print(f\"Train Macro-F1 (50k): {train_f1:.4f}\")\n",
        "print(f\"Val Macro-F1: {val_f1:.4f}\")\n",
        "print(f\"Train-Val Gap: {gap:.2f}%\")\n",
        "\n",
        "# Store baseline results\n",
        "baseline_results = {\n",
        "    \"train_samples\": DATASET_CONFIG['subsample_size'],\n",
        "    \"train_f1\": float(train_f1),\n",
        "    \"val_f1\": float(val_f1),\n",
        "    \"gap_pct\": float(gap),\n",
        "    \"hyperparameters\": PHASE1_HP\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BASELINE: PER-GENUS PERFORMANCE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n=== Baseline Per-Genus Performance ===\")\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(\n",
        "    y_val,\n",
        "    y_val_pred,\n",
        "    target_names=label_encoder.classes_,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_baseline_genus = pd.DataFrame(report).T\n",
        "df_baseline_genus = df_baseline_genus.iloc[:-3]  # Remove avg rows\n",
        "\n",
        "# Add sample counts\n",
        "genus_counts_val = pd.Series(y_val_raw).value_counts()\n",
        "df_baseline_genus['samples'] = df_baseline_genus.index.map(genus_counts_val)\n",
        "\n",
        "# Sort by F1\n",
        "df_baseline_genus = df_baseline_genus.sort_values('f1-score', ascending=False)\n",
        "\n",
        "print(df_baseline_genus[['precision', 'recall', 'f1-score', 'samples']].to_string())\n",
        "\n",
        "# Save\n",
        "df_baseline_genus.to_csv(OUTPUT_DATA / \"baseline_genus_performance.csv\")\n",
        "print(\"\\n‚úÖ Saved: baseline_genus_performance.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Experiment 4.2: Hyperparameter Optimization (Subsample)\n",
        "\n",
        "**Zweck:** Systematische HP-Tuning f√ºr Gap-Reduction auf 50k Subsample.\n",
        "\n",
        "**Methodik:**\n",
        "- Grid Search √ºber Regularization (reg_alpha, reg_lambda, min_child_weight)\n",
        "- Sampling parameters (subsample, colsample_bytree)\n",
        "- 3-Fold Spatial Block CV auf 50k Subsample\n",
        "- Metric: Macro-F1\n",
        "\n",
        "**Goal:** Gap < 30%, Val F1 ‚â• 0.60 (auf Subsample)\n",
        "\n",
        "**Expected Runtime:** ~30-45 min (243 combinations √ó 3 folds auf 50k)\n",
        "\n",
        "**Rationale:** Schnelles HP-Tuning; optimierte HPs sind f√ºr verschiedene Sample-Gr√∂√üen generalisierbar.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HYPERPARAMETER TUNING - GRID SEARCH\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=== Starting Hyperparameter Optimization (50k Subsample) ===\")\n",
        "print(f\"Grid Size: {np.prod([len(v) for v in HP_TUNING_GRID.values()])} combinations\")\n",
        "print(f\"CV Folds: {GRID_SEARCH_CONFIG['cv']}\")\n",
        "print(f\"Train Samples: {len(X_train_sub_scaled):,}\")\n",
        "print(\"\\n‚è≥ Expected runtime: ~30-45 min...\\n\")\n",
        "\n",
        "# Base model for grid search\n",
        "base_model = xgb.XGBClassifier(\n",
        "    random_state=RANDOM_SEED,\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"mlogloss\"\n",
        ")\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=base_model,\n",
        "    param_grid=HP_TUNING_GRID,\n",
        "    **GRID_SEARCH_CONFIG\n",
        ")\n",
        "\n",
        "# Fit on subsample\n",
        "start_time = datetime.now()\n",
        "grid_search.fit(X_train_sub_scaled, y_train_sub)\n",
        "elapsed_time = (datetime.now() - start_time).total_seconds() / 60\n",
        "\n",
        "print(f\"\\n‚úÖ Grid Search Complete ({elapsed_time:.1f} minutes)\")\n",
        "print(f\"\\nBest CV F1: {grid_search.best_score_:.4f}\")\n",
        "print(f\"\\nBest Hyperparameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param:20s}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EVALUATE OPTIMIZED MODEL\n",
        "# =============================================================================\n",
        "\n",
        "# Best model from grid search\n",
        "optimized_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_opt = optimized_model.predict(X_train_sub_scaled)\n",
        "y_val_pred_opt = optimized_model.predict(X_val_scaled)\n",
        "\n",
        "# Metrics\n",
        "train_f1_opt = f1_score(y_train_sub, y_train_pred_opt, average='macro')\n",
        "val_f1_opt = f1_score(y_val, y_val_pred_opt, average='macro')\n",
        "gap_opt = (train_f1_opt - val_f1_opt) / train_f1_opt * 100\n",
        "\n",
        "print(\"\\n=== Optimized Model Performance (50k Subsample) ===\")\n",
        "print(f\"Train Macro-F1 (50k): {train_f1_opt:.4f}\")\n",
        "print(f\"Val Macro-F1: {val_f1_opt:.4f}\")\n",
        "print(f\"Train-Val Gap: {gap_opt:.2f}%\")\n",
        "\n",
        "# Improvement vs Baseline\n",
        "f1_improvement = (val_f1_opt - val_f1) * 100\n",
        "gap_reduction = gap - gap_opt\n",
        "\n",
        "print(\"\\n=== Improvement vs Baseline ===\")\n",
        "print(f\"Œî Val F1: {f1_improvement:+.2f}pp\")\n",
        "print(f\"Œî Gap: {gap_reduction:+.2f}pp (reduction)\")\n",
        "\n",
        "# Store optimized results\n",
        "optimized_results = {\n",
        "    \"train_samples\": DATASET_CONFIG['subsample_size'],\n",
        "    \"train_f1\": float(train_f1_opt),\n",
        "    \"val_f1\": float(val_f1_opt),\n",
        "    \"gap_pct\": float(gap_opt),\n",
        "    \"cv_f1\": float(grid_search.best_score_),\n",
        "    \"hyperparameters\": grid_search.best_params_,\n",
        "    \"improvement_vs_baseline\": {\n",
        "        \"delta_val_f1_pp\": float(f1_improvement),\n",
        "        \"delta_gap_pp\": float(gap_reduction)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save config\n",
        "with open(OUTPUT_METADATA / \"berlin_xgboost_optimized.json\", 'w') as f:\n",
        "    json.dump(optimized_results, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Saved: berlin_xgboost_optimized.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Experiment 4.3: Sample Size Optimization\n",
        "\n",
        "**Zweck:** Identifikation der optimalen Trainings-Sample-Gr√∂√üe mit optimierten HPs.\n",
        "\n",
        "**Methodik:**\n",
        "- Optimized HPs aus Experiment 4.2\n",
        "- Variable Sample Sizes: 25k, 50k, 100k, 150k, 200k, 242k (Full)\n",
        "- Metric: Val F1, Train-Val Gap, Training Time\n",
        "\n",
        "**Expected:**\n",
        "- Diminishing returns ab ~150-200k samples\n",
        "- Gap reduction mit mehr samples\n",
        "- Optimal: Balance zwischen Performance & Effizienz\n",
        "\n",
        "**Outputs:**\n",
        "- `sample_size_optimization.csv`\n",
        "- `sample_size_curve.png` (F1 & Gap vs Sample Size)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAMPLE SIZE OPTIMIZATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=== Sample Size Optimization ===\")\n",
        "print(f\"Testing sizes: {SAMPLE_SIZES}\")\n",
        "print(f\"Using optimized HPs from Experiment 4.2\\n\")\n",
        "\n",
        "sample_size_results = []\n",
        "\n",
        "for n_samples in SAMPLE_SIZES:\n",
        "    print(f\"\\n--- Training with {n_samples:,} samples ---\")\n",
        "    \n",
        "    # Create subsample\n",
        "    if n_samples < len(df_train):\n",
        "        df_sample = df_train.sample(n=n_samples, random_state=RANDOM_SEED)\n",
        "    else:\n",
        "        df_sample = df_train\n",
        "    \n",
        "    # Prepare features\n",
        "    X_sample = df_sample[SELECTED_FEATURES].values\n",
        "    y_sample_raw = df_sample['genus_latin'].values\n",
        "    y_sample = label_encoder.transform(y_sample_raw)\n",
        "    \n",
        "    # Normalize\n",
        "    scaler_temp = StandardScaler()\n",
        "    X_sample_scaled = scaler_temp.fit_transform(X_sample)\n",
        "    X_val_scaled_temp = scaler_temp.transform(X_val)\n",
        "    \n",
        "    # Train with optimized HPs\n",
        "    model_temp = xgb.XGBClassifier(**grid_search.best_params_)\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    model_temp.fit(X_sample_scaled, y_sample)\n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    # Evaluate\n",
        "    y_train_pred_temp = model_temp.predict(X_sample_scaled)\n",
        "    y_val_pred_temp = model_temp.predict(X_val_scaled_temp)\n",
        "    \n",
        "    train_f1_temp = f1_score(y_sample, y_train_pred_temp, average='macro')\n",
        "    val_f1_temp = f1_score(y_val, y_val_pred_temp, average='macro')\n",
        "    gap_temp = (train_f1_temp - val_f1_temp) / train_f1_temp * 100\n",
        "    \n",
        "    # Store results\n",
        "    result = {\n",
        "        'n_samples': n_samples,\n",
        "        'train_f1': train_f1_temp,\n",
        "        'val_f1': val_f1_temp,\n",
        "        'gap_pct': gap_temp,\n",
        "        'training_time_s': training_time\n",
        "    }\n",
        "    sample_size_results.append(result)\n",
        "    \n",
        "    print(f\"Train F1: {train_f1_temp:.4f}\")\n",
        "    print(f\"Val F1: {val_f1_temp:.4f}\")\n",
        "    print(f\"Gap: {gap_temp:.2f}%\")\n",
        "    print(f\"Training Time: {training_time:.1f}s\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_sample_size = pd.DataFrame(sample_size_results)\n",
        "\n",
        "# Save\n",
        "df_sample_size.to_csv(OUTPUT_DATA / \"sample_size_optimization.csv\", index=False)\n",
        "print(\"\\n‚úÖ Saved: sample_size_optimization.csv\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n=== Sample Size Optimization Results ===\")\n",
        "print(df_sample_size.to_string(index=False))\n",
        "\n",
        "# Find optimal sample size (best val_f1 with reasonable training time)\n",
        "optimal_idx = df_sample_size['val_f1'].idxmax()\n",
        "optimal_samples = df_sample_size.loc[optimal_idx, 'n_samples']\n",
        "optimal_val_f1 = df_sample_size.loc[optimal_idx, 'val_f1']\n",
        "\n",
        "print(f\"\\nüéØ Optimal Sample Size: {optimal_samples:,}\")\n",
        "print(f\"   Val F1: {optimal_val_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE SAMPLE SIZE CURVE\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Val F1 vs Sample Size\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(df_sample_size['n_samples'] / 1000, df_sample_size['val_f1'], \n",
        "         marker='o', linewidth=2, markersize=8, color='steelblue')\n",
        "ax1.axvline(optimal_samples / 1000, color='red', linestyle='--', alpha=0.7, label='Optimal')\n",
        "ax1.set_xlabel('Training Samples (thousands)', fontweight='bold')\n",
        "ax1.set_ylabel('Validation F1-Score', fontweight='bold')\n",
        "ax1.set_title('Performance vs Sample Size', fontweight='bold', pad=10)\n",
        "ax1.grid(alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: Train-Val Gap vs Sample Size\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(df_sample_size['n_samples'] / 1000, df_sample_size['gap_pct'], \n",
        "         marker='s', linewidth=2, markersize=8, color='coral')\n",
        "ax2.axhline(30, color='green', linestyle='--', alpha=0.7, label='Target (30%)')\n",
        "ax2.set_xlabel('Training Samples (thousands)', fontweight='bold')\n",
        "ax2.set_ylabel('Train-Val Gap (%)', fontweight='bold')\n",
        "ax2.set_title('Overfitting vs Sample Size', fontweight='bold', pad=10)\n",
        "ax2.grid(alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "# Plot 3: Training Time vs Sample Size\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(df_sample_size['n_samples'] / 1000, df_sample_size['training_time_s'], \n",
        "         marker='^', linewidth=2, markersize=8, color='forestgreen')\n",
        "ax3.set_xlabel('Training Samples (thousands)', fontweight='bold')\n",
        "ax3.set_ylabel('Training Time (seconds)', fontweight='bold')\n",
        "ax3.set_title('Training Time vs Sample Size', fontweight='bold', pad=10)\n",
        "ax3.grid(alpha=0.3)\n",
        "\n",
        "# Plot 4: Train vs Val F1\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(df_sample_size['n_samples'] / 1000, df_sample_size['train_f1'], \n",
        "         marker='o', linewidth=2, markersize=8, label='Train F1', color='steelblue')\n",
        "ax4.plot(df_sample_size['n_samples'] / 1000, df_sample_size['val_f1'], \n",
        "         marker='s', linewidth=2, markersize=8, label='Val F1', color='coral')\n",
        "ax4.set_xlabel('Training Samples (thousands)', fontweight='bold')\n",
        "ax4.set_ylabel('F1-Score', fontweight='bold')\n",
        "ax4.set_title('Train vs Val Performance', fontweight='bold', pad=10)\n",
        "ax4.legend()\n",
        "ax4.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_PLOTS / \"sample_size_curve.png\", dpi=PUBLICATION_STYLE['dpi_export'], bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Saved: sample_size_curve.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Experiment 4.4: Final Training & Genus Analysis\n",
        "\n",
        "**Zweck:** Final model training mit optimized HPs und optimaler Sample-Gr√∂√üe.\n",
        "\n",
        "**Methodik:**\n",
        "- Best HPs from Experiment 4.2\n",
        "- Optimal Sample Size from Experiment 4.3\n",
        "- Full evaluation: Confusion Matrix, Per-Genus Metrics\n",
        "\n",
        "**Outputs:**\n",
        "- Trained model (`berlin_xgboost_model.pkl`)\n",
        "- Confusion Matrix (CSV + PNG)\n",
        "- Per-Genus Performance (CSV + PNG)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAMPLE SIZE OPTIMIZATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=== Sample Size Optimization ===\")\n",
        "print(f\"Testing sizes: {SAMPLE_SIZES}\")\n",
        "print(f\"Using optimized HPs from Experiment 4.2\\n\")\n",
        "\n",
        "sample_size_results = []\n",
        "\n",
        "for n_samples in SAMPLE_SIZES:\n",
        "    print(f\"\\n--- Training with {n_samples:,} samples ---\")\n",
        "    \n",
        "    # Create subsample\n",
        "    if n_samples < len(df_train):\n",
        "        df_sample = df_train.sample(n=n_samples, random_state=RANDOM_SEED)\n",
        "    else:\n",
        "        df_sample = df_train\n",
        "    \n",
        "    # Prepare features\n",
        "    X_sample = df_sample[SELECTED_FEATURES].values\n",
        "    y_sample_raw = df_sample['genus_latin'].values\n",
        "    y_sample = label_encoder.transform(y_sample_raw)\n",
        "    \n",
        "    # Normalize\n",
        "    scaler_temp = StandardScaler()\n",
        "    X_sample_scaled = scaler_temp.fit_transform(X_sample)\n",
        "    X_val_scaled_temp = scaler_temp.transform(X_val)\n",
        "    \n",
        "    # Train with optimized HPs\n",
        "    model_temp = xgb.XGBClassifier(**grid_search.best_params_)\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    model_temp.fit(X_sample_scaled, y_sample)\n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    # Evaluate\n",
        "    y_train_pred_temp = model_temp.predict(X_sample_scaled)\n",
        "    y_val_pred_temp = model_temp.predict(X_val_scaled_temp)\n",
        "    \n",
        "    train_f1_temp = f1_score(y_sample, y_train_pred_temp, average='macro')\n",
        "    val_f1_temp = f1_score(y_val, y_val_pred_temp, average='macro')\n",
        "    gap_temp = (train_f1_temp - val_f1_temp) / train_f1_temp * 100\n",
        "    \n",
        "    # Store results\n",
        "    result = {\n",
        "        'n_samples': n_samples,\n",
        "        'train_f1': train_f1_temp,\n",
        "        'val_f1': val_f1_temp,\n",
        "        'gap_pct': gap_temp,\n",
        "        'training_time_s': training_time\n",
        "    }\n",
        "    sample_size_results.append(result)\n",
        "    \n",
        "    print(f\"Train F1: {train_f1_temp:.4f}\")\n",
        "    print(f\"Val F1: {val_f1_temp:.4f}\")\n",
        "    print(f\"Gap: {gap_temp:.2f}%\")\n",
        "    print(f\"Training Time: {training_time:.1f}s\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_sample_size = pd.DataFrame(sample_size_results)\n",
        "\n",
        "# Save\n",
        "df_sample_size.to_csv(OUTPUT_DATA / \"sample_size_optimization.csv\", index=False)\n",
        "print(\"\\n‚úÖ Saved: sample_size_optimization.csv\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n=== Sample Size Optimization Results ===\")\n",
        "print(df_sample_size.to_string(index=False))\n",
        "\n",
        "# Find optimal sample size (best val_f1 with reasonable training time)\n",
        "optimal_idx = df_sample_size['val_f1'].idxmax()\n",
        "optimal_samples = df_sample_size.loc[optimal_idx, 'n_samples']\n",
        "optimal_val_f1 = df_sample_size.loc[optimal_idx, 'val_f1']\n",
        "\n",
        "print(f\"\\nüéØ Optimal Sample Size: {optimal_samples:,}\")\n",
        "print(f\"   Val F1: {optimal_val_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE SAMPLE SIZE CURVE\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Val F1 vs Sample Size\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(df_sample_size['n_samples'] / 1000, df_sample_size['val_f1'], \n",
        "         marker='o', linewidth=2, markersize=8, color='steelblue')\n",
        "ax1.axvline(optimal_samples / 1000, color='red', linestyle='--', alpha=0.7, label='Optimal')\n",
        "ax1.set_xlabel('Training Samples (thousands)', fontweight='bold')\n",
        "ax1.set_ylabel('Validation F1-Score', fontweight='bold')\n",
        "ax1.set_title('Performance vs Sample Size', fontweight='bold', pad=10)\n",
        "ax1.grid(alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: Train-Val Gap vs Sample Size\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(df_sample_size['n_samples'] / 1000, df_sample_size['gap_pct'], \n",
        "         marker='s', linewidth=2, markersize=8, color='coral')\n",
        "ax2.axhline(30, color='green', linestyle='--', alpha=0.7, label='Target (30%)')\n",
        "ax2.set_xlabel('Training Samples (thousands)', fontweight='bold')\n",
        "ax2.set_ylabel('Train-Val Gap (%)', fontweight='bold')\n",
        "ax2.set_title('Overfitting vs Sample Size', fontweight='bold', pad=10)\n",
        "ax2.grid(alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "# Plot 3: Training Time vs Sample Size\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(df_sample_size['n_samples'] / 1000, df_sample_size['training_time_s'], \n",
        "         marker='^', linewidth=2, markersize=8, color='forestgreen')\n",
        "ax3.set_xlabel('Training Samples (thousands)', fontweight='bold')\n",
        "ax3.set_ylabel('Training Time (seconds)', fontweight='bold')\n",
        "ax3.set_title('Training Time vs Sample Size', fontweight='bold', pad=10)\n",
        "ax3.grid(alpha=0.3)\n",
        "\n",
        "# Plot 4: Train vs Val F1\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(df_sample_size['n_samples'] / 1000, df_sample_size['train_f1'], \n",
        "         marker='o', linewidth=2, markersize=8, label='Train F1', color='steelblue')\n",
        "ax4.plot(df_sample_size['n_samples'] / 1000, df_sample_size['val_f1'], \n",
        "         marker='s', linewidth=2, markersize=8, label='Val F1', color='coral')\n",
        "ax4.set_xlabel('Training Samples (thousands)', fontweight='bold')\n",
        "ax4.set_ylabel('F1-Score', fontweight='bold')\n",
        "ax4.set_title('Train vs Val Performance', fontweight='bold', pad=10)\n",
        "ax4.legend()\n",
        "ax4.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / \"sample_size_curve.png\", dpi=PUBLICATION_STYLE['dpi_export'], bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Saved: sample_size_curve.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Experiment 4.4: Final Training & Genus Analysis\n",
        "\n",
        "**Zweck:** Final model training mit optimized HPs und optimaler Sample-Gr√∂√üe.\n",
        "\n",
        "**Methodik:**\n",
        "- Best HPs from Experiment 4.2\n",
        "- Optimal Sample Size from Experiment 4.3\n",
        "- Full evaluation: Confusion Matrix, Per-Genus Metrics\n",
        "\n",
        "**Outputs:**\n",
        "- Trained model (`berlin_xgboost_model.pkl`)\n",
        "- Confusion Matrix (CSV + PNG)\n",
        "- Per-Genus Performance (CSV + PNG)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL MODEL: PER-GENUS PERFORMANCE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=== Final Model: Per-Genus Performance ===\")\n",
        "\n",
        "# Classification report\n",
        "report_opt = classification_report(\n",
        "    y_val,\n",
        "    y_val_pred_opt,\n",
        "    target_names=label_encoder.classes_,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_genus_final = pd.DataFrame(report_opt).T\n",
        "df_genus_final = df_genus_final.iloc[:-3]  # Remove avg rows\n",
        "\n",
        "# Add sample counts\n",
        "df_genus_final['samples'] = df_genus_final.index.map(genus_counts_val)\n",
        "\n",
        "# Sort by F1\n",
        "df_genus_final = df_genus_final.sort_values('f1-score', ascending=False)\n",
        "\n",
        "print(df_genus_final[['precision', 'recall', 'f1-score', 'samples']].to_string())\n",
        "\n",
        "# Save\n",
        "df_genus_final.to_csv(OUTPUT_DATA / \"genus_performance_berlin.csv\")\n",
        "print(\"\\n‚úÖ Saved: genus_performance_berlin.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFUSION MATRIX\n",
        "# =============================================================================\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_val, y_val_pred_opt)\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=label_encoder.classes_,\n",
        "    columns=label_encoder.classes_\n",
        ")\n",
        "\n",
        "# Save\n",
        "cm_df.to_csv(OUTPUT_DATA / \"confusion_matrix_berlin.csv\")\n",
        "print(\"‚úÖ Saved: confusion_matrix_berlin.csv\")\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Normalize for percentages\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(\n",
        "    cm_norm,\n",
        "    annot=True,\n",
        "    fmt='.2f',\n",
        "    cmap='Blues',\n",
        "    xticklabels=label_encoder.classes_,\n",
        "    yticklabels=label_encoder.classes_,\n",
        "    ax=ax,\n",
        "    cbar_kws={'label': 'Proportion'}\n",
        ")\n",
        "\n",
        "ax.set_xlabel('Predicted Genus', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('True Genus', fontsize=12, fontweight='bold')\n",
        "ax.set_title(\n",
        "    f'Confusion Matrix - Berlin (Val F1: {val_f1_opt:.4f})',\n",
        "    fontsize=14,\n",
        "    fontweight='bold',\n",
        "    pad=20\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / \"confusion_matrix_berlin.png\", dpi=PUBLICATION_STYLE['dpi_export'], bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Saved: confusion_matrix_berlin.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GENUS PERFORMANCE VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Prepare data\n",
        "df_viz = df_genus_final.sort_values('f1-score', ascending=True)\n",
        "\n",
        "# Bar plot\n",
        "bars = ax.barh(df_viz.index, df_viz['f1-score'], color='steelblue', alpha=0.8)\n",
        "\n",
        "# Add sample sizes as text\n",
        "for i, (idx, row) in enumerate(df_viz.iterrows()):\n",
        "    ax.text(\n",
        "        row['f1-score'] + 0.01,\n",
        "        i,\n",
        "        f\"n={int(row['samples']):,}\",\n",
        "        va='center',\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "ax.set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Genus', fontsize=12, fontweight='bold')\n",
        "ax.set_title(\n",
        "    'Per-Genus F1-Score (Berlin Validation Set)',\n",
        "    fontsize=14,\n",
        "    fontweight='bold',\n",
        "    pad=20\n",
        ")\n",
        "ax.set_xlim(0, 1)\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / \"genus_performance_comparison.png\", dpi=PUBLICATION_STYLE['dpi_export'], bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Saved: genus_performance_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAMPLE SIZE CORRELATION ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n=== Sample Size vs Performance Correlation ===\")\n",
        "\n",
        "# Correlation\n",
        "corr = df_genus_final['samples'].corr(df_genus_final['f1-score'])\n",
        "print(f\"Pearson Correlation (samples vs F1): {corr:.3f}\")\n",
        "\n",
        "# Interpretation\n",
        "if corr > 0.5:\n",
        "    print(\"‚Üí Strong positive correlation: Larger genera perform better\")\n",
        "elif corr > 0.2:\n",
        "    print(\"‚Üí Moderate positive correlation: Sample size has some effect\")\n",
        "else:\n",
        "    print(\"‚Üí Weak correlation: Performance driven by separability, not sample size\")\n",
        "\n",
        "# Scatter plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax.scatter(\n",
        "    df_genus_final['samples'],\n",
        "    df_genus_final['f1-score'],\n",
        "    s=100,\n",
        "    alpha=0.7,\n",
        "    color='steelblue'\n",
        ")\n",
        "\n",
        "# Add genus labels\n",
        "for idx, row in df_genus_final.iterrows():\n",
        "    ax.annotate(\n",
        "        idx,\n",
        "        (row['samples'], row['f1-score']),\n",
        "        xytext=(5, 5),\n",
        "        textcoords='offset points',\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "ax.set_xlabel('Sample Size (Validation Set)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
        "ax.set_title(\n",
        "    f'Sample Size vs Performance (r={corr:.3f})',\n",
        "    fontsize=14,\n",
        "    fontweight='bold',\n",
        "    pad=20\n",
        ")\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / \"sample_size_correlation.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Saved: sample_size_correlation.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Experiment 4.4: Feature Importance Analysis\n",
        "\n",
        "**Zweck:** Identifikation kritischer Features f√ºr Genus-Klassifikation.\n",
        "\n",
        "**Methoden:**\n",
        "1. **XGBoost Native Gain:** Built-in feature importance (split gain)\n",
        "2. **Permutation Importance:** Feature-wise Œî F1 durch Shuffling\n",
        "\n",
        "**Expected:**\n",
        "- Top Features: Spectral bands (B08, B11, B12) + Vegetation Indices (NDVI, EVI)\n",
        "- Temporal Pattern: Jun-Aug Features wichtiger als Apr/Nov\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# NATIVE FEATURE IMPORTANCE (GAIN)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=== XGBoost Native Feature Importance (Gain) ===\")\n",
        "\n",
        "# Get feature importance\n",
        "importance_gain = optimized_model.feature_importances_\n",
        "\n",
        "# Create DataFrame\n",
        "df_importance_gain = pd.DataFrame({\n",
        "    'feature': SELECTED_FEATURES,\n",
        "    'gain': importance_gain\n",
        "}).sort_values('gain', ascending=False)\n",
        "\n",
        "# Display Top-20\n",
        "print(\"\\nTop-20 Features (by Gain):\")\n",
        "print(df_importance_gain.head(20).to_string(index=False))\n",
        "\n",
        "# Save\n",
        "df_importance_gain.to_csv(OUTPUT_DATA / \"feature_importance_gain.csv\", index=False)\n",
        "print(\"\\n‚úÖ Saved: feature_importance_gain.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PERMUTATION IMPORTANCE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n=== Permutation Importance (Œî F1) ===\")\n",
        "print(\"‚è≥ Computing permutation importance (this may take ~10 minutes)...\\n\")\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(\n",
        "    optimized_model,\n",
        "    X_val_scaled,\n",
        "    y_val,\n",
        "    n_repeats=10,\n",
        "    random_state=RANDOM_SEED,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Create DataFrame\n",
        "df_importance_perm = pd.DataFrame({\n",
        "    'feature': SELECTED_FEATURES,\n",
        "    'importance': perm_importance.importances_mean,\n",
        "    'std': perm_importance.importances_std\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Display Top-20\n",
        "print(\"Top-20 Features (by Permutation):\")\n",
        "print(df_importance_perm.head(20).to_string(index=False))\n",
        "\n",
        "# Save\n",
        "df_importance_perm.to_csv(OUTPUT_DATA / \"feature_importance_permutation.csv\", index=False)\n",
        "print(\"\\n‚úÖ Saved: feature_importance_permutation.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FEATURE IMPORTANCE VISUALIZATION (TOP-20)\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Plot 1: Gain-based\n",
        "df_viz_gain = df_importance_gain.head(20).sort_values('gain', ascending=True)\n",
        "axes[0].barh(df_viz_gain['feature'], df_viz_gain['gain'], color='steelblue', alpha=0.8)\n",
        "axes[0].set_xlabel('Gain', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Feature', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Top-20 Features (XGBoost Gain)', fontsize=13, fontweight='bold')\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Plot 2: Permutation-based\n",
        "df_viz_perm = df_importance_perm.head(20).sort_values('importance', ascending=True)\n",
        "axes[1].barh(df_viz_perm['feature'], df_viz_perm['importance'], color='coral', alpha=0.8)\n",
        "axes[1].set_xlabel('Œî F1 (Permutation)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Feature', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Top-20 Features (Permutation Importance)', fontsize=13, fontweight='bold')\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.suptitle(\n",
        "    'Feature Importance Analysis (Berlin)',\n",
        "    fontsize=15,\n",
        "    fontweight='bold',\n",
        "    y=1.00\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / \"feature_importance_top20.png\", dpi=PUBLICATION_STYLE['dpi_export'], bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Saved: feature_importance_top20.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9. Experiment 4.5: Temporal Analysis\n",
        "\n",
        "**Zweck:** Identifikation kritischer Monate f√ºr die Klassifikation.\n",
        "\n",
        "**Methodik:**\n",
        "- Ablation: Entferne alle Features eines Monats (Apr-Nov)\n",
        "- Trainiere Model ohne diesen Monat\n",
        "- Œî F1 = F1_full - F1_ablated\n",
        "- Higher Œî F1 = kritischerer Monat\n",
        "\n",
        "**Expected:** Jun-Aug kritisch (peak vegetation season)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEMPORAL ABLATION ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=== Temporal Ablation Analysis ===\")\n",
        "print(\"‚è≥ Training 8 models (one per month ablation, ~10 minutes)...\\n\")\n",
        "\n",
        "temporal_results = []\n",
        "\n",
        "for month in MONTHS:\n",
        "    print(f\"\\nAblating {month}...\")\n",
        "    \n",
        "    # Identify features to exclude (all features containing month number)\n",
        "    month_num = MONTH_MAPPING[month]\n",
        "    month_pattern = f\"_{month_num:02d}_\"  # e.g., \"_04_\" for April\n",
        "    \n",
        "    ablated_features = [\n",
        "        feat for feat in SELECTED_FEATURES \n",
        "        if month_pattern not in feat\n",
        "    ]\n",
        "    \n",
        "    excluded_count = len(SELECTED_FEATURES) - len(ablated_features)\n",
        "    print(f\"  Excluding {excluded_count} features\")\n",
        "    \n",
        "    # Get feature indices\n",
        "    feature_indices = [SELECTED_FEATURES.index(f) for f in ablated_features]\n",
        "    \n",
        "    # Subset data\n",
        "    X_train_ablated = X_train_scaled[:, feature_indices]\n",
        "    X_val_ablated = X_val_scaled[:, feature_indices]\n",
        "    \n",
        "    # Train model\n",
        "    model_ablated = xgb.XGBClassifier(**grid_search.best_params_)\n",
        "    model_ablated.fit(X_train_ablated, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_val_pred_ablated = model_ablated.predict(X_val_ablated)\n",
        "    f1_ablated = f1_score(y_val, y_val_pred_ablated, average='macro')\n",
        "    \n",
        "    # Compute delta\n",
        "    delta_f1 = val_f1_opt - f1_ablated\n",
        "    \n",
        "    print(f\"  F1 without {month}: {f1_ablated:.4f}\")\n",
        "    print(f\"  Œî F1: {delta_f1:+.4f}\")\n",
        "    \n",
        "    temporal_results.append({\n",
        "        'month': month,\n",
        "        'month_num': month_num,\n",
        "        'f1_ablated': f1_ablated,\n",
        "        'delta_f1': delta_f1,\n",
        "        'features_excluded': excluded_count\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_temporal = pd.DataFrame(temporal_results)\n",
        "df_temporal = df_temporal.sort_values('delta_f1', ascending=False)\n",
        "\n",
        "print(\"\\n=== Temporal Importance Ranking ===\")\n",
        "print(df_temporal.to_string(index=False))\n",
        "\n",
        "# Save\n",
        "df_temporal.to_csv(OUTPUT_DATA / \"temporal_importance.csv\", index=False)\n",
        "print(\"\\n‚úÖ Saved: temporal_importance.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEMPORAL CONTRIBUTION VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Sort by month order for logical x-axis\n",
        "df_viz_temporal = df_temporal.sort_values('month_num')\n",
        "\n",
        "# Bar plot\n",
        "bars = ax.bar(\n",
        "    df_viz_temporal['month'],\n",
        "    df_viz_temporal['delta_f1'],\n",
        "    color='seagreen',\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "# Highlight top-3 months\n",
        "top3_months = df_temporal.nlargest(3, 'delta_f1')['month'].values\n",
        "for i, month in enumerate(df_viz_temporal['month']):\n",
        "    if month in top3_months:\n",
        "        bars[i].set_color('crimson')\n",
        "        bars[i].set_alpha(0.9)\n",
        "\n",
        "ax.set_xlabel('Month', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Œî F1 (Performance Drop)', fontsize=12, fontweight='bold')\n",
        "ax.set_title(\n",
        "    'Temporal Feature Contribution (Month Ablation)',\n",
        "    fontsize=14,\n",
        "    fontweight='bold',\n",
        "    pad=20\n",
        ")\n",
        "ax.axhline(0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add legend\n",
        "ax.text(\n",
        "    0.98, 0.98,\n",
        "    'Red = Top-3 most critical months',\n",
        "    transform=ax.transAxes,\n",
        "    fontsize=10,\n",
        "    verticalalignment='top',\n",
        "    horizontalalignment='right',\n",
        "    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / \"temporal_contribution.png\", dpi=PUBLICATION_STYLE['dpi_export'], bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Saved: temporal_contribution.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 10. Experiment 4.6: Tree Type Analysis\n",
        "\n",
        "**Zweck:** Untersuchung systematischer Performance-Unterschiede zwischen Deciduous/Coniferous.\n",
        "\n",
        "**Note:** Alle 6 Genera im Datensatz sind deciduous ‚Üí Analyse limitiert auf genus-level characteristics.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TREE TYPE ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=== Tree Type Analysis ===\")\n",
        "\n",
        "# Add tree type to genus performance\n",
        "df_genus_final['tree_type'] = df_genus_final.index.map(TREE_TYPE_MAPPING)\n",
        "\n",
        "# Count by type\n",
        "type_counts = df_genus_final['tree_type'].value_counts()\n",
        "print(f\"\\nTree Type Distribution:\")\n",
        "for tree_type, count in type_counts.items():\n",
        "    print(f\"  {tree_type}: {count} genera\")\n",
        "\n",
        "# Average performance by type\n",
        "type_performance = df_genus_final.groupby('tree_type')[['precision', 'recall', 'f1-score']].mean()\n",
        "\n",
        "print(\"\\nAverage Performance by Tree Type:\")\n",
        "print(type_performance.to_string())\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Note: All 6 genera in dataset are deciduous.\")\n",
        "print(\"   Coniferous analysis requires extended dataset (PINUS, PICEA, etc.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GENUS-SPECIFIC CHARACTERISTICS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n=== Genus-Specific Characteristics ===\")\n",
        "\n",
        "# Identify best and worst performing genera\n",
        "best_genus = df_genus_final['f1-score'].idxmax()\n",
        "worst_genus = df_genus_final['f1-score'].idxmin()\n",
        "\n",
        "print(f\"\\nBest Performing: {best_genus}\")\n",
        "print(f\"  F1: {df_genus_final.loc[best_genus, 'f1-score']:.4f}\")\n",
        "print(f\"  Samples: {int(df_genus_final.loc[best_genus, 'samples']):,}\")\n",
        "\n",
        "print(f\"\\nWorst Performing: {worst_genus}\")\n",
        "print(f\"  F1: {df_genus_final.loc[worst_genus, 'f1-score']:.4f}\")\n",
        "print(f\"  Samples: {int(df_genus_final.loc[worst_genus, 'samples']):,}\")\n",
        "\n",
        "# Most confused pairs (from confusion matrix)\n",
        "print(\"\\n=== Most Confused Genus Pairs ===\")\n",
        "\n",
        "# Get off-diagonal elements\n",
        "confusion_pairs = []\n",
        "for i, true_genus in enumerate(label_encoder.classes_):\n",
        "    for j, pred_genus in enumerate(label_encoder.classes_):\n",
        "        if i != j:\n",
        "            confusion_pairs.append({\n",
        "                'true': true_genus,\n",
        "                'predicted': pred_genus,\n",
        "                'count': cm[i, j],\n",
        "                'pct_of_true': cm[i, j] / cm[i, :].sum() * 100\n",
        "            })\n",
        "\n",
        "df_confusion_pairs = pd.DataFrame(confusion_pairs)\n",
        "df_confusion_pairs = df_confusion_pairs.sort_values('count', ascending=False)\n",
        "\n",
        "print(\"\\nTop-10 Confusion Pairs:\")\n",
        "print(df_confusion_pairs.head(10)[['true', 'predicted', 'count', 'pct_of_true']].to_string(index=False))\n",
        "\n",
        "# Save\n",
        "df_confusion_pairs.to_csv(OUTPUT_DIR / \"confusion_pairs.csv\", index=False)\n",
        "print(\"\\n‚úÖ Saved: confusion_pairs.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 11. Model Export\n",
        "\n",
        "**Zweck:** Persistierung des finalen optimierten Models f√ºr zuk√ºnftige Verwendung.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAVE FINAL MODEL\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=== Saving Final Model ===\")\n",
        "\n",
        "# Package model with preprocessing objects\n",
        "model_package = {\n",
        "    'model': optimized_model,\n",
        "    'scaler': scaler,\n",
        "    'label_encoder': label_encoder,\n",
        "    'selected_features': SELECTED_FEATURES,\n",
        "    'hyperparameters': grid_search.best_params_,\n",
        "    'performance': {\n",
        "        'train_f1': float(train_f1_opt),\n",
        "        'val_f1': float(val_f1_opt),\n",
        "        'gap_pct': float(gap_opt)\n",
        "    },\n",
        "    'metadata': {\n",
        "        'training_date': datetime.now().isoformat(),\n",
        "        'dataset': 'Berlin (242k train, 62k val)',\n",
        "        'configuration': '20m-Edge, Top-50 Features, No CHM',\n",
        "        'phase': 'Phase 4: Berlin Maximum Performance'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save as pickle\n",
        "model_path = OUTPUT_METADATA / \"berlin_xgboost_model.pkl\"\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(model_package, f)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved: {model_path.name}\")\n",
        "print(f\"   Size: {model_path.stat().st_size / 1e6:.1f} MB\")\n",
        "\n",
        "# Save metadata separately as JSON (human-readable)\n",
        "metadata = {\n",
        "    'hyperparameters': grid_search.best_params_,\n",
        "    'performance': model_package['performance'],\n",
        "    'metadata': model_package['metadata'],\n",
        "    'selected_features': SELECTED_FEATURES[:10] + ['...']  # First 10 only\n",
        "}\n",
        "\n",
        "with open(OUTPUT_DIR / \"model_metadata.json\", 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Saved: model_metadata.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 12. Summary & Insights\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 4: BERLIN MAXIMUM PERFORMANCE - FINAL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n[1] PERFORMANCE IMPROVEMENTS\")\n",
        "print(f\"    Phase 1 Baseline:  {val_f1:.4f} F1 (Gap: {gap:.2f}%)\")\n",
        "print(f\"    Phase 4 Optimized: {val_f1_opt:.4f} F1 (Gap: {gap_opt:.2f}%)\")\n",
        "print(f\"    Improvement:       {f1_improvement:+.2f}pp F1 ({f1_improvement/val_f1*100:+.1f}%)\")\n",
        "print(f\"    Gap Reduction:     {gap_reduction:+.2f}pp\")\n",
        "\n",
        "print(\"\\n[2] OPTIMIZED HYPERPARAMETERS\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"    {param:20s}: {value}\")\n",
        "\n",
        "print(\"\\n[3] PER-GENUS PERFORMANCE\")\n",
        "for genus in df_genus_final.index:\n",
        "    f1 = df_genus_final.loc[genus, 'f1-score']\n",
        "    samples = int(df_genus_final.loc[genus, 'samples'])\n",
        "    print(f\"    {genus:12s}: {f1:.4f} F1 (n={samples:,})\")\n",
        "\n",
        "print(\"\\n[4] FEATURE IMPORTANCE (TOP-5)\")\n",
        "for i, row in df_importance_gain.head(5).iterrows():\n",
        "    print(f\"    {i+1}. {row['feature']:30s} (Gain: {row['gain']:.4f})\")\n",
        "\n",
        "print(\"\\n[5] TEMPORAL IMPORTANCE (TOP-3)\")\n",
        "for i, row in df_temporal.head(3).iterrows():\n",
        "    print(f\"    {i+1}. {row['month']:5s} (Œî F1: {row['delta_f1']:+.4f})\")\n",
        "\n",
        "print(\"\\n[6] MOST CONFUSED PAIRS (TOP-3)\")\n",
        "for i, row in df_confusion_pairs.head(3).iterrows():\n",
        "    print(f\"    {row['true']} ‚Üí {row['predicted']}: {int(row['count'])} ({row['pct_of_true']:.1f}%)\")\n",
        "\n",
        "print(\"\\n[7] KEY INSIGHTS\")\n",
        "\n",
        "# Sample size correlation insight\n",
        "corr = df_genus_final['samples'].corr(df_genus_final['f1-score'])\n",
        "if corr > 0.5:\n",
        "    print(f\"    ‚Ä¢ Sample size strongly correlated with F1 (r={corr:.3f})\")\n",
        "    print(\"      ‚Üí Smaller genera (SORBUS, FRAXINUS) need more data\")\n",
        "elif corr < 0.2:\n",
        "    print(f\"    ‚Ä¢ Performance NOT driven by sample size (r={corr:.3f})\")\n",
        "    print(\"      ‚Üí Genus separability more important than data volume\")\n",
        "\n",
        "# Temporal insight\n",
        "critical_months = df_temporal.nlargest(3, 'delta_f1')['month'].values\n",
        "print(f\"    ‚Ä¢ Critical months: {', '.join(critical_months)}\")\n",
        "print(\"      ‚Üí Peak vegetation season most informative\")\n",
        "\n",
        "# Confusion insight\n",
        "top_confusion = df_confusion_pairs.iloc[0]\n",
        "print(f\"    ‚Ä¢ Most confused: {top_confusion['true']} ‚Üí {top_confusion['predicted']}\")\n",
        "print(\"      ‚Üí Morphologically similar genera hard to separate\")\n",
        "\n",
        "print(\"\\n[8] OUTPUTS SAVED\")\n",
        "print(\"    ‚Ä¢ berlin_xgboost_optimized.json\")\n",
        "print(\"    ‚Ä¢ berlin_xgboost_model.pkl\")\n",
        "print(\"    ‚Ä¢ genus_performance_berlin.csv\")\n",
        "print(\"    ‚Ä¢ confusion_matrix_berlin.csv/png\")\n",
        "print(\"    ‚Ä¢ feature_importance_gain.csv\")\n",
        "print(\"    ‚Ä¢ feature_importance_permutation.csv\")\n",
        "print(\"    ‚Ä¢ temporal_importance.csv\")\n",
        "print(\"    ‚Ä¢ confusion_pairs.csv\")\n",
        "print(\"    ‚Ä¢ [8 PNG visualizations]\")\n",
        "\n",
        "print(\"\\n[9] NEXT STEPS\")\n",
        "print(\"    ‚Ä¢ Use optimized HP config for extended dataset (10+ genera)\")\n",
        "print(\"    ‚Ä¢ Focus data collection on identified critical months (Jun-Aug)\")\n",
        "print(\"    ‚Ä¢ Address most confused genus pairs with additional features\")\n",
        "print(\"    ‚Ä¢ Test model on Hamburg/Rostock for transfer validation\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ PHASE 4 COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}