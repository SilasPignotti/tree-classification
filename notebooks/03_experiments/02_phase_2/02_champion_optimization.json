{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: Champion Optimization & Analysis\n",
        "\n",
        "**Project:** Tree Species Classification - Cross-City Transfer Learning  \n",
        "**Phase:** 2.2 (Champion Optimization)  \n",
        "**Date:** 20. Januar 2026  \n",
        "**Author:** Silas Pignotti\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 1. OVERVIEW & METHODOLOGY"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Purpose\n",
        "\n",
        "This notebook optimizes the **Champion Configuration** (1D-CNN + Combined Training Setup) from Phase 2.1 to achieve maximum Zero-Shot Transfer Performance on Rostock.\n",
        "\n",
        "**Primary Goals:**\n",
        "1. **Full Data Training:** Utilize all available Berlin+Hamburg data (308k samples)\n",
        "2. **Hyperparameter Optimization:** Tune 1D-CNN architecture for optimal transfer\n",
        "3. **Training Data Scaling:** Identify optimal data quantity (10k → 308k)\n",
        "4. **Genus-Level Analysis:** Understand per-class transfer performance\n",
        "5. **Final Model Training:** Train best configuration for downstream use\n",
        "\n",
        "**Research Questions:**\n",
        "- Q1: How much does Full Data improve over 36k subsample?\n",
        "- Q2: Which HP configuration maximizes transfer robustness?\n",
        "- Q3: Where does performance saturate with more training data?\n",
        "- Q4: Which genera transfer well/poorly?\n",
        "- Q5: What is the maximum achievable Zero-Shot Transfer F1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Workflow\n",
        "\n",
        "```\n",
        "[SECTION 5: HP-TUNING]\n",
        "├── 5.1: Baseline Full Data (Phase 2.1 Config)\n",
        "├── 5.2: Learning Rate Tuning\n",
        "├── 5.3: Architecture Tuning (Layers, Filters)\n",
        "├── 5.4: Regularization Tuning (Dropout, Batch Size)\n",
        "└── 5.5: Select Best HP Configuration\n",
        "\n",
        "    ↓\n",
        "\n",
        "[SECTION 6: TRAINING CURVE]\n",
        "├── 6.1: Data Scaling Experiment (10k → 308k)\n",
        "├── 6.2: Performance vs. Sample Size\n",
        "└── 6.3: Saturation Analysis\n",
        "\n",
        "    ↓\n",
        "\n",
        "[SECTION 7: FINAL MODEL]\n",
        "├── 7.1: Train Best Configuration on Full Data\n",
        "├── 7.2: Genus-Level Performance\n",
        "├── 7.3: Confusion Matrix Analysis\n",
        "└── 7.4: Model Export\n",
        "\n",
        "    ↓\n",
        "\n",
        "[OUTPUT: optimized_model.pth, final_performance.json]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Expected Outputs\n",
        "\n",
        "| File | Type | Description |\n",
        "|------|------|-------------|\n",
        "| `optimized_model.pth` | PyTorch | Best 1D-CNN model weights |\n",
        "| `best_hp_config.json` | JSON | Optimal hyperparameters |\n",
        "| `hp_tuning_results.csv` | CSV | All HP experiments |\n",
        "| `training_curve.csv` | CSV | Performance vs sample size |\n",
        "| `genus_performance.csv` | CSV | Per-genus F1 scores |\n",
        "| `confusion_matrix.png` | PNG | Final model confusion matrix |\n",
        "| `training_curve_plot.png` | PNG | Scaling curve visualization |\n",
        "| `final_performance.json` | JSON | Complete evaluation metrics |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n", "\n", "## 2. SETUP & IMPORTS"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 2.1 Packages & Environment"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn --quiet\n",
        "!pip install torch torchvision --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "# Scikit-learn\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    f1_score, accuracy_score, classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Imports successful\")\n",
        "print(f\"  - scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"  - torch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 2.2 Visualization & Utility Functions"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Publication style setup\n",
        "PUBLICATION_STYLE = {\n",
        "    'style': 'seaborn-v0_8-whitegrid',\n",
        "    'figsize': (12, 7),\n",
        "    'dpi_export': 300,\n",
        "}\n",
        "\n",
        "def setup_publication_style():\n",
        "    plt.rcdefaults()\n",
        "    plt.style.use(PUBLICATION_STYLE['style'])\n",
        "    sns.set_palette('Set2')\n",
        "    plt.rcParams['figure.figsize'] = PUBLICATION_STYLE['figsize']\n",
        "    plt.rcParams['savefig.dpi'] = PUBLICATION_STYLE['dpi_export']\n",
        "    print(\"Publication Style configured\")\n",
        "\n",
        "setup_publication_style()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "\n",
        "def log_step(step_name):\n",
        "    \"\"\"Log execution step with timestamp.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"[{timestamp}] {step_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "def evaluate_model(y_true, y_pred, y_train_pred=None, y_train_true=None, prefix=\"\"):\n",
        "    \"\"\"Calculate comprehensive metrics for model evaluation.\"\"\"\n",
        "    \n",
        "    # Test metrics\n",
        "    test_f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    test_f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "    test_accuracy = accuracy_score(y_true, y_pred)\n",
        "    \n",
        "    metrics = {\n",
        "        f'{prefix}test_f1_macro': test_f1_macro,\n",
        "        f'{prefix}test_f1_weighted': test_f1_weighted,\n",
        "        f'{prefix}test_accuracy': test_accuracy,\n",
        "    }\n",
        "    \n",
        "    # Training metrics (if provided)\n",
        "    if y_train_pred is not None and y_train_true is not None:\n",
        "        train_f1_macro = f1_score(y_train_true, y_train_pred, average='macro')\n",
        "        gap = (train_f1_macro - test_f1_macro) * 100\n",
        "        \n",
        "        metrics[f'{prefix}train_f1_macro'] = train_f1_macro\n",
        "        metrics[f'{prefix}train_test_gap_pct'] = gap\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def compute_genus_wise_f1(y_true, y_pred, label_encoder):\n",
        "    \"\"\"Compute F1 score per genus.\"\"\"\n",
        "    genus_f1 = {}\n",
        "    \n",
        "    for genus_idx, genus_name in enumerate(label_encoder.classes_):\n",
        "        # Binary F1 for this genus\n",
        "        y_true_binary = (y_true == genus_idx).astype(int)\n",
        "        y_pred_binary = (y_pred == genus_idx).astype(int)\n",
        "        \n",
        "        if y_true_binary.sum() > 0:\n",
        "            genus_f1[genus_name] = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
        "        else:\n",
        "            genus_f1[genus_name] = np.nan\n",
        "    \n",
        "    return genus_f1\n",
        "\n",
        "def save_json(data, filepath):\n",
        "    \"\"\"Save dictionary as JSON with nice formatting.\"\"\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(data, f, indent=2, default=str)\n",
        "    print(f\"Saved: {filepath}\")\n",
        "\n",
        "print(\"Utility functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n", "\n", "## 3. CONFIGURATION & PARAMETERS"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 3.1 Paths"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\"/content/drive/MyDrive/Studium/Geoinformation/Module/Projektarbeit\")\n",
        "\n",
        "# Input Data (From Phase 1 Data Prep)\n",
        "INPUT_DATA_DIR = BASE_DIR / \"data/03_experiments/01_phase_1/00_data_preparation/data\"\n",
        "BERLIN_TRAIN_FILE = INPUT_DATA_DIR / \"berlin_20m_edge_top50_train.parquet\"\n",
        "HAMBURG_TRAIN_FILE = INPUT_DATA_DIR / \"hamburg_20m_edge_top50_train.parquet\"\n",
        "ROSTOCK_ZERO_SHOT_FILE = INPUT_DATA_DIR / \"rostock_20m_edge_top50_zero_shot.parquet\"\n",
        "\n",
        "# Phase 2.1 Results (Champion Selection)\n",
        "PHASE_2_1_METADATA_DIR = BASE_DIR / \"data/03_experiments/02_phase_2/01_transfer_evaluation/metadata\"\n",
        "SELECTED_TRANSFER_SETUP_FILE = PHASE_2_1_METADATA_DIR / \"selected_transfer_setup.json\"\n",
        "\n",
        "# Phase 0 Metadata (Selected Features)\n",
        "PHASE_0_METADATA_DIR = BASE_DIR / \"data/03_experiments/00_phase_0/03_experiment_feature_reduction/metadata\"\n",
        "SELECTED_FEATURES_FILE = PHASE_0_METADATA_DIR / \"selected_features.json\"\n",
        "\n",
        "# Experiment Output (Phase 2.2 - Champion Optimization)\n",
        "OUTPUT_DIR = BASE_DIR / \"data/03_experiments/02_phase_2/02_champion_optimization\"\n",
        "OUTPUT_DATA = OUTPUT_DIR / \"data\"\n",
        "OUTPUT_METADATA = OUTPUT_DIR / \"metadata\"\n",
        "OUTPUT_MODELS = OUTPUT_DIR / \"models\"\n",
        "OUTPUT_PLOTS = OUTPUT_DIR / \"plots\"\n",
        "\n",
        "# Create directories\n",
        "for path in [OUTPUT_DATA, OUTPUT_METADATA, OUTPUT_MODELS, OUTPUT_PLOTS]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Input data directory: {INPUT_DATA_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 3.2 Experiment Parameters"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Configuration\n",
        "LABEL_COL = 'genus_latin'\n",
        "METADATA_COLS = ['block_id', 'outlier_level', 'tree_id', 'tree_type', 'city']\n",
        "\n",
        "EXPERIMENT_PARAMS = {\n",
        "    # Dataset\n",
        "    'dataset': '20m_edge',\n",
        "    'features': 'top_50',\n",
        "    \n",
        "    # Random Seeds\n",
        "    'random_seed': 42,\n",
        "    \n",
        "    # Training Data Scaling Curve\n",
        "    'scaling_sizes': [10000, 50000, 100000, 200000, 'full'],\n",
        "    \n",
        "    # HP-Tuning Budget\n",
        "    'hp_tuning_trials': 15,\n",
        "    \n",
        "    # Phase 2.1 Baseline (for comparison)\n",
        "    'phase_2_1_baseline_f1': 0.3459,  # 1D-CNN Combined 36k\n",
        "}\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(EXPERIMENT_PARAMS['random_seed'])\n",
        "torch.manual_seed(EXPERIMENT_PARAMS['random_seed'])\n",
        "\n",
        "# Display parameters\n",
        "print(\"Experiment Parameters:\")\n",
        "print(\"-\" * 70)\n",
        "for key, value in EXPERIMENT_PARAMS.items():\n",
        "    print(f\"  {key:<30} {str(value):<20}\")\n",
        "\n",
        "print(f\"\\nData Configuration:\")\n",
        "print(f\"  Label Column: {LABEL_COL}\")\n",
        "print(f\"  Metadata: {METADATA_COLS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 3.3 Load Phase 2.1 Champion Configuration"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"LOADING: Phase 2.1 Champion Configuration\")\n",
        "\n",
        "# Load selected transfer setup\n",
        "with open(SELECTED_TRANSFER_SETUP_FILE, 'r') as f:\n",
        "    phase_2_1_selection = json.load(f)\n",
        "\n",
        "print(\"Phase 2.1 Champion:\")\n",
        "print(f\"  Algorithm:      {phase_2_1_selection['champion']['algorithm']}\")\n",
        "print(f\"  Training Setup: {phase_2_1_selection['champion']['training_setup']}\")\n",
        "print(f\"  Test F1:        {phase_2_1_selection['champion']['test_f1_rostock']:.4f}\")\n",
        "print(f\"  Transfer Loss:  {phase_2_1_selection['transfer_loss']['loss_pp']:.4f} ({phase_2_1_selection['transfer_loss']['loss_pct']:.1f}%)\")\n",
        "\n",
        "print(f\"\\nBaseline to Beat: {phase_2_1_selection['champion']['test_f1_rostock']:.4f} (36k subsample)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 3.4 Define 1D-CNN Model Architecture"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1D-CNN Model Architecture (configurable)\n",
        "class Configurable1DCNN(nn.Module):\n",
        "    def __init__(self, num_features, num_classes, config):\n",
        "        \"\"\"\n",
        "        Configurable 1D-CNN for HP-Tuning.\n",
        "        \n",
        "        config: dict with keys\n",
        "          - num_conv_layers: int (2-4)\n",
        "          - filters: list of ints (e.g., [32, 64] or [64, 128, 256])\n",
        "          - kernel_size: int (3 or 5)\n",
        "          - dense_units: int (64, 128, 256)\n",
        "          - dropout: float (0.2-0.5)\n",
        "        \"\"\"\n",
        "        super(Configurable1DCNN, self).__init__()\n",
        "        \n",
        "        # Extract config\n",
        "        num_conv_layers = config.get('num_conv_layers', 2)\n",
        "        filters = config.get('filters', [32, 64])\n",
        "        kernel_size = config.get('kernel_size', 3)\n",
        "        dense_units = config.get('dense_units', 128)\n",
        "        dropout = config.get('dropout', 0.3)\n",
        "        \n",
        "        # Build Conv Layers\n",
        "        layers = []\n",
        "        in_channels = 1\n",
        "        \n",
        "        for i in range(num_conv_layers):\n",
        "            out_channels = filters[i] if i < len(filters) else filters[-1]\n",
        "            \n",
        "            layers.append(nn.Conv1d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                padding=kernel_size//2\n",
        "            ))\n",
        "            layers.append(nn.BatchNorm1d(out_channels))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.MaxPool1d(kernel_size=2))\n",
        "            \n",
        "            in_channels = out_channels\n",
        "        \n",
        "        self.features = nn.Sequential(*layers)\n",
        "        \n",
        "        # Compute flattened size dynamically\n",
        "        dummy_input = torch.zeros(1, 1, num_features)\n",
        "        dummy_output = self.features(dummy_input)\n",
        "        linear_input_size = dummy_output.view(1, -1).size(1)\n",
        "        \n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(linear_input_size, dense_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dense_units, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "print(\"Configurable 1D-CNN Model defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n", "\n", "## 4. DATA LOADING"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 4.1 Load Selected Features"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"LOADING: Selected Features from Phase 0\")\n",
        "\n",
        "# Load Selected Features\n",
        "try:\n",
        "    with open(SELECTED_FEATURES_FILE, 'r') as f:\n",
        "        feature_data = json.load(f)\n",
        "        if isinstance(feature_data, list):\n",
        "            selected_features = feature_data\n",
        "        elif isinstance(feature_data, dict):\n",
        "            if 'selected_features' in feature_data:\n",
        "                selected_features = feature_data['selected_features']\n",
        "            elif 'features' in feature_data:\n",
        "                selected_features = feature_data['features']\n",
        "            else:\n",
        "                selected_features = list(feature_data.keys())\n",
        "        else:\n",
        "            selected_features = None\n",
        "    \n",
        "    if selected_features:\n",
        "        print(f\"Loaded {len(selected_features)} selected features from Phase 0\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading selected features: {e}\")\n",
        "    selected_features = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 4.2 Load Full Combined Training Data (Berlin + Hamburg)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"LOADING: Full Combined Training Data (Berlin + Hamburg)\")\n",
        "\n",
        "# Load Berlin Full\n",
        "df_berlin = pd.read_parquet(BERLIN_TRAIN_FILE)\n",
        "print(f\"Loaded Berlin: {df_berlin.shape}\")\n",
        "\n",
        "# Load Hamburg Full\n",
        "df_hamburg = pd.read_parquet(HAMBURG_TRAIN_FILE)\n",
        "print(f\"Loaded Hamburg: {df_hamburg.shape}\")\n",
        "\n",
        "# Filter to selected features\n",
        "if selected_features:\n",
        "    def filter_features(df):\n",
        "        available_features = [f for f in selected_features if f in df.columns]\n",
        "        cols_to_keep = [LABEL_COL] + [c for c in METADATA_COLS if c in df.columns] + available_features\n",
        "        cols_to_keep = list(dict.fromkeys(cols_to_keep))\n",
        "        return df[cols_to_keep]\n",
        "    \n",
        "    df_berlin = filter_features(df_berlin)\n",
        "    df_hamburg = filter_features(df_hamburg)\n",
        "\n",
        "print(f\"Filtered Berlin: {df_berlin.shape}\")\n",
        "print(f\"Filtered Hamburg: {df_hamburg.shape}\")\n",
        "\n",
        "# Combine\n",
        "df_combined_full = pd.concat([df_berlin, df_hamburg], ignore_index=True)\n",
        "print(f\"\\nCombined Full: {df_combined_full.shape}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df_combined_full[LABEL_COL].value_counts())\n",
        "\n",
        "if 'city' in df_combined_full.columns:\n",
        "    print(f\"\\nCity distribution:\")\n",
        "    print(df_combined_full['city'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 4.3 Load Rostock Test Data"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"LOADING: Rostock Zero-Shot Test Data\")\n",
        "\n",
        "# Load Rostock\n",
        "df_rostock = pd.read_parquet(ROSTOCK_ZERO_SHOT_FILE)\n",
        "print(f\"Loaded Rostock: {df_rostock.shape}\")\n",
        "\n",
        "# Filter to selected features\n",
        "if selected_features:\n",
        "    available_features = [f for f in selected_features if f in df_rostock.columns]\n",
        "    cols_to_keep = [LABEL_COL] + [c for c in METADATA_COLS if c in df_rostock.columns] + available_features\n",
        "    cols_to_keep = list(dict.fromkeys(cols_to_keep))\n",
        "    df_rostock = df_rostock[cols_to_keep]\n",
        "\n",
        "print(f\"Filtered Rostock: {df_rostock.shape}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df_rostock[LABEL_COL].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 4.4 Prepare Feature-Target Split"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"PREPARING: Feature-Target Split\")\n",
        "\n",
        "def prepare_dataset(df_train, df_test, label_col, metadata_cols):\n",
        "    \"\"\"Prepare X, y for train and test sets.\"\"\"\n",
        "    \n",
        "    # Columns to drop\n",
        "    cols_to_drop = [label_col] + [c for c in metadata_cols if c in df_train.columns]\n",
        "    \n",
        "    # Features\n",
        "    X_train = df_train.drop(columns=cols_to_drop).values\n",
        "    X_test = df_test.drop(columns=cols_to_drop).values\n",
        "    \n",
        "    # Labels\n",
        "    y_train = df_train[label_col].values\n",
        "    y_test = df_test[label_col].values\n",
        "    \n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(np.concatenate([y_train, y_test]))\n",
        "    \n",
        "    y_train_encoded = label_encoder.transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "    \n",
        "    return X_train, y_train_encoded, X_test, y_test_encoded, label_encoder\n",
        "\n",
        "# Prepare full dataset\n",
        "X_combined_full, y_combined_full, X_rostock, y_rostock, label_encoder = prepare_dataset(\n",
        "    df_combined_full, df_rostock, LABEL_COL, METADATA_COLS\n",
        ")\n",
        "\n",
        "print(f\"Full Training Data: {X_combined_full.shape}\")\n",
        "print(f\"Test Data (Rostock): {X_rostock.shape}\")\n",
        "print(f\"\\nClasses: {label_encoder.classes_}\")\n",
        "print(f\"Number of Classes: {len(label_encoder.classes_)}\")\n",
        "\n",
        "num_features = X_combined_full.shape[1]\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "print(f\"\\nNum Features: {num_features}\")\n",
        "print(f\"Num Classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n", "\n", "## 5. HYPERPARAMETER TUNING"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 5.1 Helper Function: Train and Evaluate CNN"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_cnn_configuration(X_train, y_train, X_test, y_test, model_config, train_config, device='cuda'):\n",
        "    \"\"\"\n",
        "    Train 1D-CNN with given configuration and evaluate on test set.\n",
        "    \n",
        "    Args:\n",
        "        X_train, y_train: Training data\n",
        "        X_test, y_test: Test data\n",
        "        model_config: dict with model architecture params\n",
        "        train_config: dict with training params (lr, batch_size, epochs)\n",
        "        device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "        dict with metrics and trained model\n",
        "    \"\"\"\n",
        "    # Scale data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Datasets\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.FloatTensor(X_train_scaled),\n",
        "        torch.LongTensor(y_train)\n",
        "    )\n",
        "    test_dataset = TensorDataset(\n",
        "        torch.FloatTensor(X_test_scaled),\n",
        "        torch.LongTensor(y_test)\n",
        "    )\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_config['batch_size'],\n",
        "        shuffle=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=train_config['batch_size'],\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    # Model\n",
        "    model = Configurable1DCNN(\n",
        "        num_features=X_train.shape[1],\n",
        "        num_classes=num_classes,\n",
        "        config=model_config\n",
        "    ).to(device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=train_config['lr'])\n",
        "    \n",
        "    # Training\n",
        "    model.train()\n",
        "    for epoch in range(train_config['epochs']):\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    \n",
        "    # Test predictions\n",
        "    test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "    \n",
        "    # Train predictions (subsample for speed)\n",
        "    train_loader_eval = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_config['batch_size'],\n",
        "        shuffle=False\n",
        "    )\n",
        "    train_preds = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in train_loader_eval:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_preds.extend(preds.cpu().numpy())\n",
        "    \n",
        "    # Metrics\n",
        "    metrics = evaluate_model(\n",
        "        y_test,\n",
        "        np.array(test_preds),\n",
        "        np.array(train_preds),\n",
        "        y_train\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'metrics': metrics,\n",
        "        'model': model,\n",
        "        'scaler': scaler,\n",
        "        'test_preds': np.array(test_preds),\n",
        "    }\n",
        "\n",
        "print(\"Helper function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 5.2 Baseline: Phase 2.1 Configuration on Full Data"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"BASELINE: Phase 2.1 Config on Full Data\")\n",
        "\n",
        "# Phase 2.1 Configuration\n",
        "baseline_model_config = {\n",
        "    'num_conv_layers': 2,\n",
        "    'filters': [32, 64],\n",
        "    'kernel_size': 3,\n",
        "    'dense_units': 128,\n",
        "    'dropout': 0.3,\n",
        "}\n",
        "\n",
        "baseline_train_config = {\n",
        "    'lr': 0.001,\n",
        "    'batch_size': 128,\n",
        "    'epochs': 30,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Train baseline\n",
        "print(f\"\\nTraining baseline with Full Data ({len(X_combined_full):,} samples)...\")\n",
        "baseline_results = train_cnn_configuration(\n",
        "    X_combined_full, y_combined_full,\n",
        "    X_rostock, y_rostock,\n",
        "    baseline_model_config,\n",
        "    baseline_train_config,\n",
        "    device\n",
        ")\n",
        "\n",
        "baseline_f1 = baseline_results['metrics']['test_f1_macro']\n",
        "\n",
        "print(f\"\\nBaseline Results (Full Data):\")\n",
        "print(f\"  Test F1:        {baseline_f1:.4f}\")\n",
        "print(f\"  Test Accuracy:  {baseline_results['metrics']['test_accuracy']:.4f}\")\n",
        "print(f\"  Train-Test Gap: {baseline_results['metrics']['train_test_gap_pct']:.2f}%\")\n",
        "\n",
        "# Compare to Phase 2.1 (36k subsample)\n",
        "phase_2_1_f1 = EXPERIMENT_PARAMS['phase_2_1_baseline_f1']\n",
        "improvement = baseline_f1 - phase_2_1_f1\n",
        "\n",
        "print(f\"\\nComparison to Phase 2.1 (36k):\")\n",
        "print(f\"  Phase 2.1: {phase_2_1_f1:.4f}\")\n",
        "print(f\"  Full Data: {baseline_f1:.4f}\")\n",
        "print(f\"  Improvement: {improvement:+.4f} ({improvement/phase_2_1_f1*100:+.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 5.3 HP-Tuning: Learning Rate"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"HP-TUNING: Learning Rate\")\n",
        "\n",
        "# Learning rates to test\n",
        "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
        "\n",
        "lr_results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nTesting LR={lr}...\")\n",
        "    \n",
        "    train_config = baseline_train_config.copy()\n",
        "    train_config['lr'] = lr\n",
        "    train_config['epochs'] = 20  # Faster for tuning\n",
        "    \n",
        "    results = train_cnn_configuration(\n",
        "        X_combined_full, y_combined_full,\n",
        "        X_rostock, y_rostock,\n",
        "        baseline_model_config,\n",
        "        train_config,\n",
        "        device\n",
        "    )\n",
        "    \n",
        "    test_f1 = results['metrics']['test_f1_macro']\n",
        "    \n",
        "    lr_results.append({\n",
        "        'lr': lr,\n",
        "        'test_f1': test_f1,\n",
        "        'test_accuracy': results['metrics']['test_accuracy'],\n",
        "        'gap': results['metrics']['train_test_gap_pct'],\n",
        "    })\n",
        "    \n",
        "    print(f\"  Test F1: {test_f1:.4f}\")\n",
        "\n",
        "df_lr = pd.DataFrame(lr_results)\n",
        "\n",
        "print(\"\\nLearning Rate Tuning Results:\")\n",
        "print(\"=\"*70)\n",
        "print(df_lr.to_string(index=False))\n",
        "\n",
        "best_lr = df_lr.sort_values('test_f1', ascending=False).iloc[0]['lr']\n",
        "print(f\"\\nBest Learning Rate: {best_lr}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 5.4 HP-Tuning: Architecture (Filters & Layers)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"HP-TUNING: Architecture (Filters & Layers)\")\n",
        "\n",
        "# Architectures to test\n",
        "architectures = [\n",
        "    {'num_conv_layers': 2, 'filters': [32, 64], 'name': 'Small (32-64)'},\n",
        "    {'num_conv_layers': 2, 'filters': [64, 128], 'name': 'Medium (64-128)'},\n",
        "    {'num_conv_layers': 3, 'filters': [32, 64, 128], 'name': 'Deep-Small (32-64-128)'},\n",
        "    {'num_conv_layers': 3, 'filters': [64, 128, 256], 'name': 'Deep-Large (64-128-256)'},\n",
        "]\n",
        "\n",
        "arch_results = []\n",
        "\n",
        "for arch in architectures:\n",
        "    print(f\"\\nTesting {arch['name']}...\")\n",
        "    \n",
        "    model_config = baseline_model_config.copy()\n",
        "    model_config['num_conv_layers'] = arch['num_conv_layers']\n",
        "    model_config['filters'] = arch['filters']\n",
        "    \n",
        "    train_config = baseline_train_config.copy()\n",
        "    train_config['lr'] = best_lr\n",
        "    train_config['epochs'] = 20\n",
        "    \n",
        "    results = train_cnn_configuration(\n",
        "        X_combined_full, y_combined_full,\n",
        "        X_rostock, y_rostock,\n",
        "        model_config,\n",
        "        train_config,\n",
        "        device\n",
        "    )\n",
        "    \n",
        "    test_f1 = results['metrics']['test_f1_macro']\n",
        "    \n",
        "    arch_results.append({\n",
        "        'architecture': arch['name'],\n",
        "        'num_layers': arch['num_conv_layers'],\n",
        "        'filters': str(arch['filters']),\n",
        "        'test_f1': test_f1,\n",
        "        'test_accuracy': results['metrics']['test_accuracy'],\n",
        "        'gap': results['metrics']['train_test_gap_pct'],\n",
        "    })\n",
        "    \n",
        "    print(f\"  Test F1: {test_f1:.4f}\")\n",
        "\n",
        "df_arch = pd.DataFrame(arch_results)\n",
        "\n",
        "print(\"\\nArchitecture Tuning Results:\")\n",
        "print(\"=\"*70)\n",
        "print(df_arch.to_string(index=False))\n",
        "\n",
        "best_arch_idx = df_arch['test_f1'].idxmax()\n",
        "best_arch = architectures[best_arch_idx]\n",
        "print(f\"\\nBest Architecture: {best_arch['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 5.5 HP-Tuning: Regularization (Dropout & Batch Size)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"HP-TUNING: Regularization (Dropout & Batch Size)\")\n",
        "\n",
        "# Regularization configs to test\n",
        "reg_configs = [\n",
        "    {'dropout': 0.2, 'batch_size': 128, 'name': 'Low Dropout (0.2), BS=128'},\n",
        "    {'dropout': 0.3, 'batch_size': 128, 'name': 'Med Dropout (0.3), BS=128'},\n",
        "    {'dropout': 0.4, 'batch_size': 128, 'name': 'High Dropout (0.4), BS=128'},\n",
        "    {'dropout': 0.3, 'batch_size': 64, 'name': 'Med Dropout (0.3), BS=64'},\n",
        "    {'dropout': 0.3, 'batch_size': 256, 'name': 'Med Dropout (0.3), BS=256'},\n",
        "]\n",
        "\n",
        "reg_results = []\n",
        "\n",
        "for config in reg_configs:\n",
        "    print(f\"\\nTesting {config['name']}...\")\n",
        "    \n",
        "    model_config = baseline_model_config.copy()\n",
        "    model_config['num_conv_layers'] = best_arch['num_conv_layers']\n",
        "    model_config['filters'] = best_arch['filters']\n",
        "    model_config['dropout'] = config['dropout']\n",
        "    \n",
        "    train_config = baseline_train_config.copy()\n",
        "    train_config['lr'] = best_lr\n",
        "    train_config['batch_size'] = config['batch_size']\n",
        "    train_config['epochs'] = 20\n",
        "    \n",
        "    results = train_cnn_configuration(\n",
        "        X_combined_full, y_combined_full,\n",
        "        X_rostock, y_rostock,\n",
        "        model_config,\n",
        "        train_config,\n",
        "        device\n",
        "    )\n",
        "    \n",
        "    test_f1 = results['metrics']['test_f1_macro']\n",
        "    \n",
        "    reg_results.append({\n",
        "        'config': config['name'],\n",
        "        'dropout': config['dropout'],\n",
        "        'batch_size': config['batch_size'],\n",
        "        'test_f1': test_f1,\n",
        "        'test_accuracy': results['metrics']['test_accuracy'],\n",
        "        'gap': results['metrics']['train_test_gap_pct'],\n",
        "    })\n",
        "    \n",
        "    print(f\"  Test F1: {test_f1:.4f}\")\n",
        "\n",
        "df_reg = pd.DataFrame(reg_results)\n",
        "\n",
        "print(\"\\nRegularization Tuning Results:\")\n",
        "print(\"=\"*70)\n",
        "print(df_reg.to_string(index=False))\n",
        "\n",
        "best_reg_idx = df_reg['test_f1'].idxmax()\n",
        "best_reg_config = reg_configs[best_reg_idx]\n",
        "print(f\"\\nBest Regularization: {best_reg_config['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 5.6 Compile Best HP Configuration"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"SELECTION: Best HP Configuration\")\n",
        "\n",
        "# Compile best config\n",
        "best_hp_config = {\n",
        "    'model': {\n",
        "        'num_conv_layers': best_arch['num_conv_layers'],\n",
        "        'filters': best_arch['filters'],\n",
        "        'kernel_size': 3,\n",
        "        'dense_units': 128,\n",
        "        'dropout': best_reg_config['dropout'],\n",
        "    },\n",
        "    'training': {\n",
        "        'lr': best_lr,\n",
        "        'batch_size': best_reg_config['batch_size'],\n",
        "        'epochs': 50,  # More epochs for final training\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BEST HP CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nModel Architecture:\")\n",
        "for key, value in best_hp_config['model'].items():\n",
        "    print(f\"  {key:<20} {value}\")\n",
        "\n",
        "print(\"\\nTraining Parameters:\")\n",
        "for key, value in best_hp_config['training'].items():\n",
        "    print(f\"  {key:<20} {value}\")\n",
        "\n",
        "# Save HP config\n",
        "save_json(best_hp_config, OUTPUT_METADATA / 'best_hp_config.json')\n",
        "\n",
        "# Save all HP tuning results\n",
        "pd.concat([\n",
        "    df_lr.assign(experiment='learning_rate'),\n",
        "    df_arch.assign(experiment='architecture'),\n",
        "    df_reg.assign(experiment='regularization')\n",
        "], ignore_index=True).to_csv(OUTPUT_DATA / 'hp_tuning_results.csv', index=False)\n",
        "\n",
        "print(\"\\nHP tuning results saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n", "\n", "## 6. TRAINING DATA SCALING CURVE"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 6.1 Train Models with Different Data Sizes"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"EXPERIMENT: Training Data Scaling Curve\")\n",
        "\n",
        "# Sample sizes to test\n",
        "sample_sizes = EXPERIMENT_PARAMS['scaling_sizes']\n",
        "\n",
        "# Replace 'full' with actual full size\n",
        "full_size = len(X_combined_full)\n",
        "sample_sizes_numeric = [s if isinstance(s, int) else full_size for s in sample_sizes]\n",
        "\n",
        "print(f\"Testing sample sizes: {sample_sizes_numeric}\")\n",
        "\n",
        "scaling_results = []\n",
        "\n",
        "for size in sample_sizes_numeric:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training with {size:,} samples...\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Subsample if needed\n",
        "    if size < full_size:\n",
        "        # Stratified sampling\n",
        "        indices = np.arange(len(X_combined_full))\n",
        "        _, selected_indices = train_test_split(\n",
        "            indices,\n",
        "            train_size=size,\n",
        "            stratify=y_combined_full,\n",
        "            random_state=EXPERIMENT_PARAMS['random_seed']\n",
        "        )\n",
        "        X_train_subset = X_combined_full[selected_indices]\n",
        "        y_train_subset = y_combined_full[selected_indices]\n",
        "    else:\n",
        "        X_train_subset = X_combined_full\n",
        "        y_train_subset = y_combined_full\n",
        "    \n",
        "    # Train with best HP\n",
        "    results = train_cnn_configuration(\n",
        "        X_train_subset, y_train_subset,\n",
        "        X_rostock, y_rostock,\n",
        "        best_hp_config['model'],\n",
        "        best_hp_config['training'],\n",
        "        device\n",
        "    )\n",
        "    \n",
        "    scaling_results.append({\n",
        "        'sample_size': size,\n",
        "        'test_f1': results['metrics']['test_f1_macro'],\n",
        "        'test_accuracy': results['metrics']['test_accuracy'],\n",
        "        'train_f1': results['metrics']['train_f1_macro'],\n",
        "        'gap': results['metrics']['train_test_gap_pct'],\n",
        "    })\n",
        "    \n",
        "    print(f\"Test F1: {results['metrics']['test_f1_macro']:.4f}\")\n",
        "\n",
        "df_scaling = pd.DataFrame(scaling_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING DATA SCALING RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(df_scaling.to_string(index=False))\n",
        "\n",
        "# Save\n",
        "df_scaling.to_csv(OUTPUT_DATA / 'training_curve.csv', index=False)\n",
        "print(\"\\nTraining curve saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 6.2 Visualize Scaling Curve"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"VISUALIZATION: Training Data Scaling Curve\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot 1: Test F1 vs Sample Size\n",
        "ax1 = axes[0]\n",
        "ax1.plot(df_scaling['sample_size'], df_scaling['test_f1'], marker='o', linewidth=2, markersize=8)\n",
        "ax1.axhline(y=phase_2_1_f1, color='red', linestyle='--', label=f'Phase 2.1 Baseline ({phase_2_1_f1:.4f})')\n",
        "ax1.set_xlabel('Training Sample Size', fontsize=12)\n",
        "ax1.set_ylabel('Test F1 Macro', fontsize=12)\n",
        "ax1.set_title('Performance vs. Training Data Size', fontsize=14, fontweight='bold')\n",
        "ax1.set_xscale('log')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# Add annotations for key points\n",
        "for _, row in df_scaling.iterrows():\n",
        "    if row['sample_size'] in [10000, full_size]:\n",
        "        ax1.annotate(\n",
        "            f\"{row['test_f1']:.4f}\",\n",
        "            xy=(row['sample_size'], row['test_f1']),\n",
        "            xytext=(10, 10),\n",
        "            textcoords='offset points',\n",
        "            fontsize=9\n",
        "        )\n",
        "\n",
        "# Plot 2: Train-Test Gap vs Sample Size\n",
        "ax2 = axes[1]\n",
        "ax2.plot(df_scaling['sample_size'], df_scaling['gap'], marker='s', linewidth=2, markersize=8, color='orange')\n",
        "ax2.axhline(y=35, color='red', linestyle='--', label='Target Gap (35%)')\n",
        "ax2.set_xlabel('Training Sample Size', fontsize=12)\n",
        "ax2.set_ylabel('Train-Test Gap (%)', fontsize=12)\n",
        "ax2.set_title('Overfitting vs. Training Data Size', fontsize=14, fontweight='bold')\n",
        "ax2.set_xscale('log')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_PLOTS / 'training_curve_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Scaling curve visualization saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 6.3 Saturation Analysis"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"ANALYSIS: Performance Saturation\")\n",
        "\n",
        "# Compute marginal gains\n",
        "df_scaling_sorted = df_scaling.sort_values('sample_size')\n",
        "df_scaling_sorted['marginal_gain'] = df_scaling_sorted['test_f1'].diff()\n",
        "df_scaling_sorted['marginal_gain_pct'] = (df_scaling_sorted['marginal_gain'] / df_scaling_sorted['test_f1'].shift(1)) * 100\n",
        "\n",
        "print(\"Marginal Gains per Data Size Increase:\")\n",
        "print(\"=\"*70)\n",
        "print(df_scaling_sorted[['sample_size', 'test_f1', 'marginal_gain', 'marginal_gain_pct']].to_string(index=False))\n",
        "\n",
        "# Find saturation point (marginal gain < 1%)\n",
        "saturation_rows = df_scaling_sorted[df_scaling_sorted['marginal_gain_pct'] < 1.0]\n",
        "\n",
        "if not saturation_rows.empty:\n",
        "    saturation_size = saturation_rows.iloc[0]['sample_size']\n",
        "    print(f\"\\nSaturation Point: ~{saturation_size:,} samples\")\n",
        "    print(f\"  (Marginal gain < 1% beyond this point)\")\n",
        "else:\n",
        "    print(f\"\\nNo saturation observed - performance still improving with more data\")\n",
        "\n",
        "# Total improvement from 10k to Full\n",
        "min_f1 = df_scaling_sorted.iloc[0]['test_f1']\n",
        "max_f1 = df_scaling_sorted.iloc[-1]['test_f1']\n",
        "total_gain = max_f1 - min_f1\n",
        "\n",
        "print(f\"\\nTotal Improvement (10k → Full):\")\n",
        "print(f\"  10k F1:    {min_f1:.4f}\")\n",
        "print(f\"  Full F1:   {max_f1:.4f}\")\n",
        "print(f\"  Gain:      {total_gain:+.4f} ({total_gain/min_f1*100:+.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n", "\n", "## 7. FINAL MODEL TRAINING & ANALYSIS"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 7.1 Train Final Optimized Model"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"FINAL MODEL: Training with Best Configuration\")\n",
        "\n",
        "print(\"Training final model with:\")\n",
        "print(f\"  - Best HP Configuration\")\n",
        "print(f\"  - Full Combined Data ({full_size:,} samples)\")\n",
        "print(f\"  - Extended Epochs ({best_hp_config['training']['epochs']})\")\n",
        "\n",
        "# Train final model\n",
        "final_results = train_cnn_configuration(\n",
        "    X_combined_full, y_combined_full,\n",
        "    X_rostock, y_rostock,\n",
        "    best_hp_config['model'],\n",
        "    best_hp_config['training'],\n",
        "    device\n",
        ")\n",
        "\n",
        "final_model = final_results['model']\n",
        "final_scaler = final_results['scaler']\n",
        "final_test_preds = final_results['test_preds']\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL MODEL PERFORMANCE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nTest Set (Rostock Zero-Shot):\")\n",
        "print(f\"  Test F1 Macro:    {final_results['metrics']['test_f1_macro']:.4f}\")\n",
        "print(f\"  Test F1 Weighted: {final_results['metrics']['test_f1_weighted']:.4f}\")\n",
        "print(f\"  Test Accuracy:    {final_results['metrics']['test_accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining Set:\")\n",
        "print(f\"  Train F1 Macro:   {final_results['metrics']['train_f1_macro']:.4f}\")\n",
        "print(f\"  Train-Test Gap:   {final_results['metrics']['train_test_gap_pct']:.2f}%\")\n",
        "\n",
        "# Comparison to Phase 2.1\n",
        "final_f1 = final_results['metrics']['test_f1_macro']\n",
        "improvement_vs_phase_2_1 = final_f1 - phase_2_1_f1\n",
        "\n",
        "print(f\"\\nImprovement vs. Phase 2.1 (36k):\")\n",
        "print(f\"  Phase 2.1:      {phase_2_1_f1:.4f}\")\n",
        "print(f\"  Final Optimized: {final_f1:.4f}\")\n",
        "print(f\"  Total Gain:     {improvement_vs_phase_2_1:+.4f} ({improvement_vs_phase_2_1/phase_2_1_f1*100:+.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 7.2 Genus-Level Performance Analysis"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"ANALYSIS: Genus-Level Performance\")\n",
        "\n",
        "# Compute per-genus F1\n",
        "genus_f1 = compute_genus_wise_f1(y_rostock, final_test_preds, label_encoder)\n",
        "\n",
        "# Get genus counts in test set\n",
        "genus_counts = {}\n",
        "for genus_idx, genus_name in enumerate(label_encoder.classes_):\n",
        "    genus_counts[genus_name] = (y_rostock == genus_idx).sum()\n",
        "\n",
        "# Compile results\n",
        "genus_results = []\n",
        "for genus in label_encoder.classes_:\n",
        "    genus_results.append({\n",
        "        'genus': genus,\n",
        "        'test_samples': genus_counts[genus],\n",
        "        'f1_score': genus_f1[genus],\n",
        "    })\n",
        "\n",
        "df_genus = pd.DataFrame(genus_results).sort_values('f1_score', ascending=False)\n",
        "\n",
        "print(\"\\nGenus-Level Performance:\")\n",
        "print(\"=\"*70)\n",
        "print(df_genus.to_string(index=False))\n",
        "\n",
        "# Identify best/worst genera\n",
        "best_genus = df_genus.iloc[0]\n",
        "worst_genus = df_genus.iloc[-1]\n",
        "\n",
        "print(f\"\\nBest Genus:  {best_genus['genus']} (F1={best_genus['f1_score']:.4f})\")\n",
        "print(f\"Worst Genus: {worst_genus['genus']} (F1={worst_genus['f1_score']:.4f})\")\n",
        "\n",
        "# Check correlation: Sample size vs F1\n",
        "correlation = df_genus[['test_samples', 'f1_score']].corr().iloc[0, 1]\n",
        "print(f\"\\nCorrelation (Sample Size ↔ F1): {correlation:.3f}\")\n",
        "\n",
        "# Save\n",
        "df_genus.to_csv(OUTPUT_DATA / 'genus_performance.csv', index=False)\n",
        "print(\"\\nGenus performance saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 7.3 Confusion Matrix"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"VISUALIZATION: Confusion Matrix\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_rostock, final_test_preds)\n",
        "\n",
        "# Normalize by row (true class)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm_normalized,\n",
        "    annot=True,\n",
        "    fmt='.2f',\n",
        "    cmap='Blues',\n",
        "    xticklabels=label_encoder.classes_,\n",
        "    yticklabels=label_encoder.classes_,\n",
        "    cbar_kws={'label': 'Recall'}\n",
        ")\n",
        "plt.xlabel('Predicted Genus', fontsize=12)\n",
        "plt.ylabel('True Genus', fontsize=12)\n",
        "plt.title('Confusion Matrix (Normalized by True Class)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_PLOTS / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Confusion matrix saved\")\n",
        "\n",
        "# Identify most confused pairs\n",
        "print(\"\\nMost Confused Genus Pairs:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "confusion_pairs = []\n",
        "for i in range(len(label_encoder.classes_)):\n",
        "    for j in range(len(label_encoder.classes_)):\n",
        "        if i != j:\n",
        "            confusion_pairs.append({\n",
        "                'true_genus': label_encoder.classes_[i],\n",
        "                'predicted_genus': label_encoder.classes_[j],\n",
        "                'confusion_rate': cm_normalized[i, j]\n",
        "            })\n",
        "\n",
        "df_confusion = pd.DataFrame(confusion_pairs).sort_values('confusion_rate', ascending=False).head(5)\n",
        "print(df_confusion.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 7.4 Model Export"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"EXPORT: Final Model & Artifacts\")\n",
        "\n",
        "# Save model weights\n",
        "torch.save(final_model.state_dict(), OUTPUT_MODELS / 'optimized_model.pth')\n",
        "print(f\"Model weights saved: optimized_model.pth\")\n",
        "\n",
        "# Save scaler\n",
        "with open(OUTPUT_MODELS / 'scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(final_scaler, f)\n",
        "print(f\"Scaler saved: scaler.pkl\")\n",
        "\n",
        "# Save label encoder\n",
        "with open(OUTPUT_MODELS / 'label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "print(f\"Label encoder saved: label_encoder.pkl\")\n",
        "\n",
        "# Create comprehensive performance report\n",
        "final_performance = {\n",
        "    'experiment_date': datetime.now().isoformat(),\n",
        "    'model': {\n",
        "        'algorithm': '1D-CNN',\n",
        "        'architecture': best_hp_config['model'],\n",
        "        'training_params': best_hp_config['training'],\n",
        "    },\n",
        "    'data': {\n",
        "        'training_setup': 'Combined (Berlin + Hamburg)',\n",
        "        'training_samples': int(full_size),\n",
        "        'test_samples': int(len(y_rostock)),\n",
        "        'num_features': int(num_features),\n",
        "        'num_classes': int(num_classes),\n",
        "    },\n",
        "    'performance': {\n",
        "        'test_f1_macro': float(final_results['metrics']['test_f1_macro']),\n",
        "        'test_f1_weighted': float(final_results['metrics']['test_f1_weighted']),\n",
        "        'test_accuracy': float(final_results['metrics']['test_accuracy']),\n",
        "        'train_f1_macro': float(final_results['metrics']['train_f1_macro']),\n",
        "        'train_test_gap_pct': float(final_results['metrics']['train_test_gap_pct']),\n",
        "    },\n",
        "    'improvements': {\n",
        "        'phase_2_1_baseline': float(phase_2_1_f1),\n",
        "        'final_optimized': float(final_f1),\n",
        "        'total_gain_pp': float(improvement_vs_phase_2_1),\n",
        "        'total_gain_pct': float(improvement_vs_phase_2_1 / phase_2_1_f1 * 100),\n",
        "    },\n",
        "    'genus_performance': df_genus.to_dict('records'),\n",
        "    'transfer_loss': {\n",
        "        'phase_1_single_city_baseline': 0.5462,\n",
        "        'phase_2_final_transfer': float(final_f1),\n",
        "        'loss_pp': float(0.5462 - final_f1),\n",
        "        'loss_pct': float((0.5462 - final_f1) / 0.5462 * 100),\n",
        "    }\n",
        "}\n",
        "\n",
        "save_json(final_performance, OUTPUT_METADATA / 'final_performance.json')\n",
        "\n",
        "print(\"\\nFinal performance report saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n", "\n", "## 8. SUMMARY & CONCLUSIONS"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 8.1 Experiment Summary"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"SUMMARY: Phase 2.2 Champion Optimization Complete\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PHASE 2.2 COMPLETE: CHAMPION OPTIMIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nExperiments Executed:\")\n",
        "print(\"  1. Baseline Full Data (Phase 2.1 Config)\")\n",
        "print(\"  2. HP-Tuning: Learning Rate (5 configs)\")\n",
        "print(\"  3. HP-Tuning: Architecture (4 configs)\")\n",
        "print(\"  4. HP-Tuning: Regularization (5 configs)\")\n",
        "print(f\"  5. Training Curve ({len(sample_sizes_numeric)} data sizes)\")\n",
        "print(\"  6. Final Model Training & Analysis\")\n",
        "\n",
        "print(\"\\nKey Findings:\")\n",
        "\n",
        "print(\"\\n  1. Full Data Impact:\")\n",
        "baseline_full_f1 = df_scaling[df_scaling['sample_size'] == full_size]['test_f1'].values[0]\n",
        "improvement_full_vs_36k = baseline_full_f1 - phase_2_1_f1\n",
        "print(f\"     36k → {full_size:,}: {improvement_full_vs_36k:+.4f} ({improvement_full_vs_36k/phase_2_1_f1*100:+.1f}%)\")\n",
        "\n",
        "print(\"\\n  2. Best HP Configuration:\")\n",
        "print(f\"     Learning Rate: {best_lr}\")\n",
        "print(f\"     Architecture:  {best_arch['name']}\")\n",
        "print(f\"     Dropout:       {best_reg_config['dropout']}\")\n",
        "print(f\"     Batch Size:    {best_reg_config['batch_size']}\")\n",
        "\n",
        "print(\"\\n  3. Training Curve:\")\n",
        "print(f\"     10k → Full: {total_gain:+.4f} ({total_gain/min_f1*100:+.1f}%)\")\n",
        "if not saturation_rows.empty:\n",
        "    print(f\"     Saturation: ~{saturation_size:,} samples\")\n",
        "else:\n",
        "    print(f\"     Saturation: Not observed (still improving)\")\n",
        "\n",
        "print(\"\\n  4. Final Performance:\")\n",
        "print(f\"     Test F1:        {final_f1:.4f}\")\n",
        "print(f\"     Best Genus:     {best_genus['genus']} ({best_genus['f1_score']:.4f})\")\n",
        "print(f\"     Worst Genus:    {worst_genus['genus']} ({worst_genus['f1_score']:.4f})\")\n",
        "print(f\"     Transfer Loss:  {final_performance['transfer_loss']['loss_pp']:.4f} ({final_performance['transfer_loss']['loss_pct']:.1f}%)\")\n",
        "\n",
        "print(\"\\n  5. Total Improvement Chain:\")\n",
        "phase_1_baseline = 0.5462\n",
        "print(f\"     Phase 1 (Berlin Single-City):       {phase_1_baseline:.4f}\")\n",
        "print(f\"     Phase 2.1 (Combined 36k):           {phase_2_1_f1:.4f} ({phase_2_1_f1 - phase_1_baseline:+.4f})\")\n",
        "print(f\"     Phase 2.2 (Optimized Full):         {final_f1:.4f} ({final_f1 - phase_2_1_f1:+.4f})\")\n",
        "print(f\"     Total Transfer Loss:                {phase_1_baseline - final_f1:.4f} ({(phase_1_baseline - final_f1)/phase_1_baseline*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nOutput Files Generated:\")\n",
        "files_to_check = {\n",
        "    'optimized_model.pth': OUTPUT_MODELS,\n",
        "    'best_hp_config.json': OUTPUT_METADATA,\n",
        "    'final_performance.json': OUTPUT_METADATA,\n",
        "    'hp_tuning_results.csv': OUTPUT_DATA,\n",
        "    'training_curve.csv': OUTPUT_DATA,\n",
        "    'genus_performance.csv': OUTPUT_DATA,\n",
        "    'confusion_matrix.png': OUTPUT_PLOTS,\n",
        "    'training_curve_plot.png': OUTPUT_PLOTS,\n",
        "}\n",
        "\n",
        "for file, folder in files_to_check.items():\n",
        "    filepath = folder / file\n",
        "    if filepath.exists():\n",
        "        print(f\"  {file}\")\n",
        "    else:\n",
        "        print(f\"  {file} (missing!)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CHAMPION MODEL READY FOR PHASE 3 (FINE-TUNING)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### 8.2 Next Steps: Phase 3 Fine-Tuning"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_step(\"NEXT PHASE: Phase 3 Fine-Tuning Strategy\")\n",
        "\n",
        "print(\"\\nPhase 3 will evaluate:\")\n",
        "print(\"  1. Rostock → Rostock Baseline (Single-City)\")\n",
        "print(\"     - Train on Rostock 17k finetune set\")\n",
        "print(\"     - Test on Rostock 7k zero-shot set\")\n",
        "print(\"     - Establishes upper bound performance\")\n",
        "\n",
        "print(\"\\n  2. Combined → Rostock Fine-Tuning\")\n",
        "print(\"     - Start from optimized model (this notebook)\")\n",
        "print(\"     - Fine-tune on Rostock 17k with frozen/unfrozen layers\")\n",
        "print(\"     - Test: How much does fine-tuning recover?\")\n",
        "\n",
        "print(\"\\n  3. Few-Shot Fine-Tuning\")\n",
        "print(\"     - Test: 10%, 25%, 50%, 100% of Rostock finetune data\")\n",
        "print(\"     - Question: Minimum data needed to match Rostock Baseline?\")\n",
        "\n",
        "print(\"\\nResearch Questions for Phase 3:\")\n",
        "print(f\"  Q1: What is max achievable F1 on Rostock? (Single-City Baseline)\")\n",
        "print(f\"  Q2: How much does fine-tuning improve Zero-Shot ({final_f1:.4f})?\")\n",
        "print(f\"  Q3: Can we match Single-City with < 100% Rostock data?\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"END OF PHASE 2.2\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## END OF NOTEBOOK\n",
        "\n",
        "**Phase 2.2 Status:** Complete  \n",
        "**Next Notebook:** `phase_3_fine_tuning.ipynb`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
